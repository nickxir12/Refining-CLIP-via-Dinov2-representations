{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd500b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:27.674876Z",
     "iopub.status.busy": "2025-04-03T13:16:27.674593Z",
     "iopub.status.idle": "2025-04-03T13:16:29.913670Z",
     "shell.execute_reply": "2025-04-03T13:16:29.912625Z"
    },
    "papermill": {
     "duration": 2.24968,
     "end_time": "2025-04-03T13:16:29.915506",
     "exception": false,
     "start_time": "2025-04-03T13:16:27.665826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/kaggle/working/open-clip'...\r\n",
      "remote: Enumerating objects: 3798, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\r\n",
      "remote: Total 3798 (delta 7), reused 15 (delta 6), pack-reused 3779 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (3798/3798), 16.87 MiB | 30.95 MiB/s, done.\r\n",
      "Resolving deltas: 100% (2274/2274), done.\r\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "username = user_secrets.get_secret(\"GITHUB_USERNAME\")\n",
    "token = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "\n",
    "repo = \"nickxir12/MyCLIP_first_repo\"\n",
    "destination = \"/kaggle/working/open-clip\"\n",
    "\n",
    "!rm -rf /kaggle/working/open-clip  \n",
    "!git clone https://{username}:{token}@github.com/{repo}.git {destination}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd6c90df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:29.932285Z",
     "iopub.status.busy": "2025-04-03T13:16:29.931943Z",
     "iopub.status.idle": "2025-04-03T13:16:29.935623Z",
     "shell.execute_reply": "2025-04-03T13:16:29.934692Z"
    },
    "papermill": {
     "duration": 0.01353,
     "end_time": "2025-04-03T13:16:29.937022",
     "exception": false,
     "start_time": "2025-04-03T13:16:29.923492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Delete the old repo (if it exists)\n",
    "# !rm -rf /kaggle/working/open-clip  \n",
    "\n",
    "# # Step 2: Clone the latest version from GitHub\n",
    "# !git clone https://github.com/nickxir12/MyCLIP_first_repo.git /kaggle/working/open-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5ca0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:29.953079Z",
     "iopub.status.busy": "2025-04-03T13:16:29.952760Z",
     "iopub.status.idle": "2025-04-03T13:16:39.776104Z",
     "shell.execute_reply": "2025-04-03T13:16:39.774955Z"
    },
    "papermill": {
     "duration": 9.833439,
     "end_time": "2025-04-03T13:16:39.777940",
     "exception": false,
     "start_time": "2025-04-03T13:16:29.944501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting braceexpand\r\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\r\n",
      "Installing collected packages: braceexpand\r\n",
      "Successfully installed braceexpand-0.1.7\r\n",
      "Collecting webdataset\r\n",
      "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset) (0.1.7)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from webdataset) (1.26.4)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from webdataset) (6.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->webdataset) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->webdataset) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->webdataset) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->webdataset) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->webdataset) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->webdataset) (2024.2.0)\r\n",
      "Downloading webdataset-0.2.111-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: webdataset\r\n",
      "Successfully installed webdataset-0.2.111\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install open_clip_torch\n",
    "!pip install braceexpand\n",
    "!pip install webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698b39a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:39.794744Z",
     "iopub.status.busy": "2025-04-03T13:16:39.794455Z",
     "iopub.status.idle": "2025-04-03T13:16:43.603236Z",
     "shell.execute_reply": "2025-04-03T13:16:43.602300Z"
    },
    "papermill": {
     "duration": 3.818973,
     "end_time": "2025-04-03T13:16:43.605013",
     "exception": false,
     "start_time": "2025-04-03T13:16:39.786040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 1)) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 2)) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 3)) (2024.11.6)\r\n",
      "Collecting ftfy (from -r /kaggle/working/open-clip/requirements.txt (line 4))\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 5)) (4.67.1)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 6)) (0.29.0)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 7)) (0.4.5)\r\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/open-clip/requirements.txt (line 8)) (1.0.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (11.0.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->-r /kaggle/working/open-clip/requirements.txt (line 4)) (0.2.13)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (2.32.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->-r /kaggle/working/open-clip/requirements.txt (line 1)) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->-r /kaggle/working/open-clip/requirements.txt (line 6)) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->-r /kaggle/working/open-clip/requirements.txt (line 2)) (2024.2.0)\r\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ftfy\r\n",
      "Successfully installed ftfy-6.3.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /kaggle/working/open-clip/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bdbf14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:43.621960Z",
     "iopub.status.busy": "2025-04-03T13:16:43.621666Z",
     "iopub.status.idle": "2025-04-03T13:16:43.625336Z",
     "shell.execute_reply": "2025-04-03T13:16:43.624742Z"
    },
    "papermill": {
     "duration": 0.013152,
     "end_time": "2025-04-03T13:16:43.626639",
     "exception": false,
     "start_time": "2025-04-03T13:16:43.613487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/working/open-clip/src\")\n",
    "sys.path.append(\"/kaggle/working/open-clip/src/open_clip\")\n",
    "sys.path.append(\"/kaggle/working/open-clip/src/open_clip_train/my_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7c2ba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:43.642655Z",
     "iopub.status.busy": "2025-04-03T13:16:43.642414Z",
     "iopub.status.idle": "2025-04-03T13:16:43.645501Z",
     "shell.execute_reply": "2025-04-03T13:16:43.644651Z"
    },
    "papermill": {
     "duration": 0.012516,
     "end_time": "2025-04-03T13:16:43.646836",
     "exception": false,
     "start_time": "2025-04-03T13:16:43.634320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import my_metrics\n",
    "# importlib.reload(my_metrics)\n",
    "# print(dir(my_metrics))  # Now should include 'batch', 'get_all_embeddings', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46633d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:43.663262Z",
     "iopub.status.busy": "2025-04-03T13:16:43.663027Z",
     "iopub.status.idle": "2025-04-03T13:16:49.806655Z",
     "shell.execute_reply": "2025-04-03T13:16:49.805947Z"
    },
    "papermill": {
     "duration": 6.153601,
     "end_time": "2025-04-03T13:16:49.808213",
     "exception": false,
     "start_time": "2025-04-03T13:16:43.654612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_metrics import compute_consistency_score,evaluate_model\n",
    "\n",
    "#Does below work?\n",
    "from my_metrics import batch,get_all_embeddings,itm_eval,compute_consistency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8702391f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:16:49.825540Z",
     "iopub.status.busy": "2025-04-03T13:16:49.825154Z",
     "iopub.status.idle": "2025-04-03T13:17:03.315662Z",
     "shell.execute_reply": "2025-04-03T13:17:03.314879Z"
    },
    "papermill": {
     "duration": 13.500736,
     "end_time": "2025-04-03T13:17:03.317378",
     "exception": false,
     "start_time": "2025-04-03T13:16:49.816642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import open_clip_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333e3f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:03.334951Z",
     "iopub.status.busy": "2025-04-03T13:17:03.334654Z",
     "iopub.status.idle": "2025-04-03T13:17:03.337863Z",
     "shell.execute_reply": "2025-04-03T13:17:03.337252Z"
    },
    "papermill": {
     "duration": 0.013115,
     "end_time": "2025-04-03T13:17:03.339182",
     "exception": false,
     "start_time": "2025-04-03T13:17:03.326067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02dd350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:03.355236Z",
     "iopub.status.busy": "2025-04-03T13:17:03.355015Z",
     "iopub.status.idle": "2025-04-03T13:17:03.512507Z",
     "shell.execute_reply": "2025-04-03T13:17:03.511553Z"
    },
    "papermill": {
     "duration": 0.167292,
     "end_time": "2025-04-03T13:17:03.514172",
     "exception": false,
     "start_time": "2025-04-03T13:17:03.346880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import open_clip\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac9d4d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:03.531466Z",
     "iopub.status.busy": "2025-04-03T13:17:03.531167Z",
     "iopub.status.idle": "2025-04-03T13:17:03.534452Z",
     "shell.execute_reply": "2025-04-03T13:17:03.533714Z"
    },
    "papermill": {
     "duration": 0.013256,
     "end_time": "2025-04-03T13:17:03.535852",
     "exception": false,
     "start_time": "2025-04-03T13:17:03.522596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5fede5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:03.552658Z",
     "iopub.status.busy": "2025-04-03T13:17:03.552392Z",
     "iopub.status.idle": "2025-04-03T13:17:03.555506Z",
     "shell.execute_reply": "2025-04-03T13:17:03.554829Z"
    },
    "papermill": {
     "duration": 0.012759,
     "end_time": "2025-04-03T13:17:03.556825",
     "exception": false,
     "start_time": "2025-04-03T13:17:03.544066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#                Preparing train datasaet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cafce2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:03.573397Z",
     "iopub.status.busy": "2025-04-03T13:17:03.573146Z",
     "iopub.status.idle": "2025-04-03T13:17:05.885012Z",
     "shell.execute_reply": "2025-04-03T13:17:05.883805Z"
    },
    "papermill": {
     "duration": 2.321637,
     "end_time": "2025-04-03T13:17:05.886480",
     "exception": false,
     "start_time": "2025-04-03T13:17:03.564843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file saved at: /kaggle/working/train_data_karpathy.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load Karpathy JSON file\n",
    "karpathy_json_path = \"/kaggle/input/karpathy-splits/dataset_flickr30k.json\"\n",
    "with open(karpathy_json_path, \"r\") as f:\n",
    "    karpathy_data = json.load(f)\n",
    "\n",
    "# Extract training set\n",
    "train_data = [item for item in karpathy_data[\"images\"] if item[\"split\"] == \"train\"]\n",
    "\n",
    "# Prepare data for CSV\n",
    "train_records = []\n",
    "for item in train_data:\n",
    "    img_filename = f\"/kaggle/input/flickr30k/Images/{item['filename']}\"\n",
    "    for sentence in item[\"sentences\"]:\n",
    "        caption = sentence[\"raw\"]\n",
    "        train_records.append({\"image\": img_filename, \"caption\": caption})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(train_records)\n",
    "\n",
    "# Save to CSV (formatted properly)\n",
    "csv_path = \"/kaggle/working/train_data_karpathy.csv\"\n",
    "df_train.to_csv(csv_path, index=True, index_label=\"id\")\n",
    "\n",
    "print(f\"✅ CSV file saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23b0e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:05.903401Z",
     "iopub.status.busy": "2025-04-03T13:17:05.903098Z",
     "iopub.status.idle": "2025-04-03T13:17:05.928252Z",
     "shell.execute_reply": "2025-04-03T13:17:05.927417Z"
    },
    "papermill": {
     "duration": 0.034913,
     "end_time": "2025-04-03T13:17:05.929492",
     "exception": false,
     "start_time": "2025-04-03T13:17:05.894579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/flickr30k/Images/1000092795.jpg</td>\n",
       "      <td>Two young guys with shaggy hair look at their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/flickr30k/Images/1000092795.jpg</td>\n",
       "      <td>Two young, White males are outside near many b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/flickr30k/Images/1000092795.jpg</td>\n",
       "      <td>Two men in green shirts are standing in a yard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/flickr30k/Images/1000092795.jpg</td>\n",
       "      <td>A man in a blue shirt standing in a garden.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/flickr30k/Images/1000092795.jpg</td>\n",
       "      <td>Two friends enjoy time spent together.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image  \\\n",
       "0  /kaggle/input/flickr30k/Images/1000092795.jpg   \n",
       "1  /kaggle/input/flickr30k/Images/1000092795.jpg   \n",
       "2  /kaggle/input/flickr30k/Images/1000092795.jpg   \n",
       "3  /kaggle/input/flickr30k/Images/1000092795.jpg   \n",
       "4  /kaggle/input/flickr30k/Images/1000092795.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  Two young guys with shaggy hair look at their ...  \n",
       "1  Two young, White males are outside near many b...  \n",
       "2    Two men in green shirts are standing in a yard.  \n",
       "3        A man in a blue shirt standing in a garden.  \n",
       "4             Two friends enjoy time spent together.  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee39c629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:05.947051Z",
     "iopub.status.busy": "2025-04-03T13:17:05.946750Z",
     "iopub.status.idle": "2025-04-03T13:17:07.217129Z",
     "shell.execute_reply": "2025-04-03T13:17:07.216400Z"
    },
    "papermill": {
     "duration": 1.280478,
     "end_time": "2025-04-03T13:17:07.218577",
     "exception": false,
     "start_time": "2025-04-03T13:17:05.938099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "karpathy_json_path = \"/kaggle/input/karpathy-splits/dataset_flickr30k.json\"\n",
    "with open(karpathy_json_path, \"r\") as f:\n",
    "    karpathy_data = json.load(f)\n",
    "\n",
    "# Extract test set\n",
    "test_data = [item for item in karpathy_data[\"images\"] if item[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46903642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:07.237548Z",
     "iopub.status.busy": "2025-04-03T13:17:07.237248Z",
     "iopub.status.idle": "2025-04-03T13:17:07.317514Z",
     "shell.execute_reply": "2025-04-03T13:17:07.316595Z"
    },
    "papermill": {
     "duration": 0.089968,
     "end_time": "2025-04-03T13:17:07.318823",
     "exception": false,
     "start_time": "2025-04-03T13:17:07.228855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9642b39b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:07.335879Z",
     "iopub.status.busy": "2025-04-03T13:17:07.335621Z",
     "iopub.status.idle": "2025-04-03T13:17:07.338515Z",
     "shell.execute_reply": "2025-04-03T13:17:07.337837Z"
    },
    "papermill": {
     "duration": 0.012525,
     "end_time": "2025-04-03T13:17:07.339777",
     "exception": false,
     "start_time": "2025-04-03T13:17:07.327252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Standard loading\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "# model = model.to(device)\n",
    "# model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "# tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948a7bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:07.356783Z",
     "iopub.status.busy": "2025-04-03T13:17:07.356541Z",
     "iopub.status.idle": "2025-04-03T13:17:09.273068Z",
     "shell.execute_reply": "2025-04-03T13:17:09.272323Z"
    },
    "papermill": {
     "duration": 1.926755,
     "end_time": "2025-04-03T13:17:09.274775",
     "exception": false,
     "start_time": "2025-04-03T13:17:07.348020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOR CYCLIP PRE TRAINED THAT I HAVE DOWNLOADED LOCALLY\n",
    "model_clip_final, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name=\"RN50\",\n",
    "    pretrained=None,  # Don't load default weights\n",
    "    precision='fp32', # or 'amp' for mixed precision\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abb1c10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:09.292384Z",
     "iopub.status.busy": "2025-04-03T13:17:09.292121Z",
     "iopub.status.idle": "2025-04-03T13:17:16.506173Z",
     "shell.execute_reply": "2025-04-03T13:17:16.505235Z"
    },
    "papermill": {
     "duration": 7.224156,
     "end_time": "2025-04-03T13:17:16.507602",
     "exception": false,
     "start_time": "2025-04-03T13:17:09.283446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-d38036936bcd>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"/kaggle/input/clip-final/clip_best.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): ModifiedResNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU(inplace=True)\n",
       "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (avgpool): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (attnpool): AttentionPool2d(\n",
       "      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the checkpoint - OG CLIP - 3M\n",
    "ckpt = torch.load(\"/kaggle/input/clip-final/clip_best.pt\", map_location=device)\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# Remove 'module.' prefix from keys\n",
    "new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load into your model\n",
    "model_clip_final.load_state_dict(new_state_dict)\n",
    "model_clip_final.to(device)\n",
    "model_clip_final.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce6e3103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:16.526881Z",
     "iopub.status.busy": "2025-04-03T13:17:16.526556Z",
     "iopub.status.idle": "2025-04-03T13:17:17.178456Z",
     "shell.execute_reply": "2025-04-03T13:17:17.177450Z"
    },
    "papermill": {
     "duration": 0.663179,
     "end_time": "2025-04-03T13:17:17.180162",
     "exception": false,
     "start_time": "2025-04-03T13:17:16.516983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, '/kaggle/working/clip_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9d0070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:17.198684Z",
     "iopub.status.busy": "2025-04-03T13:17:17.198386Z",
     "iopub.status.idle": "2025-04-03T13:17:17.304551Z",
     "shell.execute_reply": "2025-04-03T13:17:17.303798Z"
    },
    "papermill": {
     "duration": 0.117211,
     "end_time": "2025-04-03T13:17:17.306313",
     "exception": false,
     "start_time": "2025-04-03T13:17:17.189102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenizer for my models\n",
    "tokenizer = open_clip.get_tokenizer('RN50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e093418d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:17.324897Z",
     "iopub.status.busy": "2025-04-03T13:17:17.324628Z",
     "iopub.status.idle": "2025-04-03T13:17:17.327662Z",
     "shell.execute_reply": "2025-04-03T13:17:17.326970Z"
    },
    "papermill": {
     "duration": 0.013524,
     "end_time": "2025-04-03T13:17:17.328844",
     "exception": false,
     "start_time": "2025-04-03T13:17:17.315320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2b94891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:17.346628Z",
     "iopub.status.busy": "2025-04-03T13:17:17.346392Z",
     "iopub.status.idle": "2025-04-03T13:17:17.349155Z",
     "shell.execute_reply": "2025-04-03T13:17:17.348540Z"
    },
    "papermill": {
     "duration": 0.012918,
     "end_time": "2025-04-03T13:17:17.350432",
     "exception": false,
     "start_time": "2025-04-03T13:17:17.337514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#                Models loaded are model_Cyclip_final and model_clip_final\n",
    "#                Will further fine them below\n",
    "                    # Will fine tune\n",
    "                    #     --> as usual\n",
    "                    #     --> with Dino regularizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09dcba58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:17.368399Z",
     "iopub.status.busy": "2025-04-03T13:17:17.368161Z",
     "iopub.status.idle": "2025-04-03T13:17:17.370888Z",
     "shell.execute_reply": "2025-04-03T13:17:17.370292Z"
    },
    "papermill": {
     "duration": 0.013146,
     "end_time": "2025-04-03T13:17:17.372211",
     "exception": false,
     "start_time": "2025-04-03T13:17:17.359065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!rm -rf checkpoints/dino_proj_fine_tuned_Clip\n",
    "#!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb89819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:17:17.390147Z",
     "iopub.status.busy": "2025-04-03T13:17:17.389844Z",
     "iopub.status.idle": "2025-04-03T22:26:25.911318Z",
     "shell.execute_reply": "2025-04-03T22:26:25.910001Z"
    },
    "papermill": {
     "duration": 32948.532293,
     "end_time": "2025-04-03T22:26:25.913159",
     "exception": false,
     "start_time": "2025-04-03T13:17:17.380866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-03 13:17:23.446715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-04-03 13:17:23.787067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-04-03 13:17:23.889965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using device: cuda\r\n",
      "['/kaggle/working/open-clip/src/open_clip_train/main.py', '--train-data', '/kaggle/working/train_data_karpathy.csv', '--name', 'dino_proj_fine_tuned_Clip', '--dataset-type', 'csv', '--csv-img-key', 'image', '--csv-caption-key', 'caption', '--csv-separator', ',', '--model', 'RN50', '--pretrained', '/kaggle/working/clip_final.pt', '--batch-size', '48', '--lr', '5e-6', '--warmup', '1000', '--epochs', '10', '--lr-scheduler', 'cosine', '--precision', 'amp', '--workers', '4', '--logs', 'logs', '--logs', 'checkpoints', '--save-frequency', '1', '--seed', '42', '--lambda_dino', '0.25', '--alpha', '0', '--use_dino_reg']\r\n",
      "use_soft_labels: False\r\n",
      "alpha: 0.0\r\n",
      "preprocessor_config.json: 100%|████████████████| 436/436 [00:00<00:00, 2.79MB/s]\r\n",
      "config.json: 100%|█████████████████████████████| 547/547 [00:00<00:00, 2.30MB/s]\r\n",
      "model.safetensors: 100%|████████████████████| 88.2M/88.2M [00:00<00:00, 196MB/s]\r\n",
      "2025-04-03,13:17:41 | INFO | Running with a single process. Device cuda.\r\n",
      "2025-04-03,13:17:41 | INFO | Loaded RN50 model config.\r\n",
      "2025-04-03,13:17:43 | INFO | Loading pretrained RN50 weights (/kaggle/working/clip_final.pt).\r\n",
      "2025-04-03,13:17:43 | INFO | Model:\r\n",
      "2025-04-03,13:17:43 | INFO | CLIP(\r\n",
      "  (visual): ModifiedResNet(\r\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\r\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "    (act1): ReLU(inplace=True)\r\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "    (act2): ReLU(inplace=True)\r\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "    (act3): ReLU(inplace=True)\r\n",
      "    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "    (layer1): Sequential(\r\n",
      "      (0): Bottleneck(\r\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\r\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): Bottleneck(\r\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (2): Bottleneck(\r\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer2): Sequential(\r\n",
      "      (0): Bottleneck(\r\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): Bottleneck(\r\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (2): Bottleneck(\r\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (3): Bottleneck(\r\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer3): Sequential(\r\n",
      "      (0): Bottleneck(\r\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (2): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (3): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (4): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (5): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layer4): Sequential(\r\n",
      "      (0): Bottleneck(\r\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "        (downsample): Sequential(\r\n",
      "          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (1): Bottleneck(\r\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "      (2): Bottleneck(\r\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act1): ReLU(inplace=True)\r\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act2): ReLU(inplace=True)\r\n",
      "        (avgpool): Identity()\r\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n",
      "        (act3): ReLU(inplace=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (attnpool): AttentionPool2d(\r\n",
      "      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n",
      "      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n",
      "      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\r\n",
      "      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (transformer): Transformer(\r\n",
      "    (resblocks): ModuleList(\r\n",
      "      (0-11): 12 x ResidualAttentionBlock(\r\n",
      "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (attn): MultiheadAttention(\r\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\r\n",
      "        )\r\n",
      "        (ls_1): Identity()\r\n",
      "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (mlp): Sequential(\r\n",
      "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\r\n",
      "          (gelu): GELU(approximate='none')\r\n",
      "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\r\n",
      "        )\r\n",
      "        (ls_2): Identity()\r\n",
      "      )\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (token_embedding): Embedding(49408, 512)\r\n",
      "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\r\n",
      ")\r\n",
      "2025-04-03,13:17:43 | INFO | Params:\r\n",
      "2025-04-03,13:17:43 | INFO |   accum_freq: 1\r\n",
      "2025-04-03,13:17:43 | INFO |   alpha: 0.0\r\n",
      "2025-04-03,13:17:43 | INFO |   aug_cfg: {}\r\n",
      "2025-04-03,13:17:43 | INFO |   batch_size: 48\r\n",
      "2025-04-03,13:17:43 | INFO |   beta1: 0.9\r\n",
      "2025-04-03,13:17:43 | INFO |   beta2: 0.999\r\n",
      "2025-04-03,13:17:43 | INFO |   cache_dir: None\r\n",
      "2025-04-03,13:17:43 | INFO |   checkpoint_path: checkpoints/dino_proj_fine_tuned_Clip/checkpoints\r\n",
      "2025-04-03,13:17:43 | INFO |   coca_caption_loss_weight: 2.0\r\n",
      "2025-04-03,13:17:43 | INFO |   coca_contrastive_loss_weight: 1.0\r\n",
      "2025-04-03,13:17:43 | INFO |   copy_codebase: False\r\n",
      "2025-04-03,13:17:43 | INFO |   csv_caption_key: caption\r\n",
      "2025-04-03,13:17:43 | INFO |   csv_img_key: image\r\n",
      "2025-04-03,13:17:43 | INFO |   csv_separator: ,\r\n",
      "2025-04-03,13:17:43 | INFO |   dataset_resampled: False\r\n",
      "2025-04-03,13:17:43 | INFO |   dataset_type: csv\r\n",
      "2025-04-03,13:17:43 | INFO |   ddp_static_graph: False\r\n",
      "2025-04-03,13:17:43 | INFO |   debug: False\r\n",
      "2025-04-03,13:17:43 | INFO |   delete_previous_checkpoint: False\r\n",
      "2025-04-03,13:17:43 | INFO |   device: cuda\r\n",
      "2025-04-03,13:17:43 | INFO |   dist_backend: None\r\n",
      "2025-04-03,13:17:43 | INFO |   dist_url: None\r\n",
      "2025-04-03,13:17:43 | INFO |   distill: False\r\n",
      "2025-04-03,13:17:43 | INFO |   distill_model: None\r\n",
      "2025-04-03,13:17:43 | INFO |   distill_pretrained: None\r\n",
      "2025-04-03,13:17:43 | INFO |   distributed: False\r\n",
      "2025-04-03,13:17:43 | INFO |   enforce_to_text: False\r\n",
      "2025-04-03,13:17:43 | INFO |   epochs: 10\r\n",
      "2025-04-03,13:17:43 | INFO |   epochs_cooldown: None\r\n",
      "2025-04-03,13:17:43 | INFO |   eps: 1e-08\r\n",
      "2025-04-03,13:17:43 | INFO |   force_custom_text: False\r\n",
      "2025-04-03,13:17:43 | INFO |   force_image_size: None\r\n",
      "2025-04-03,13:17:43 | INFO |   force_patch_dropout: None\r\n",
      "2025-04-03,13:17:43 | INFO |   force_quick_gelu: False\r\n",
      "2025-04-03,13:17:43 | INFO |   gather_with_grad: False\r\n",
      "2025-04-03,13:17:43 | INFO |   grad_checkpointing: False\r\n",
      "2025-04-03,13:17:43 | INFO |   grad_clip_norm: None\r\n",
      "2025-04-03,13:17:43 | INFO |   horovod: False\r\n",
      "2025-04-03,13:17:43 | INFO |   image_interpolation: None\r\n",
      "2025-04-03,13:17:43 | INFO |   image_mean: None\r\n",
      "2025-04-03,13:17:43 | INFO |   image_resize_mode: None\r\n",
      "2025-04-03,13:17:43 | INFO |   image_std: None\r\n",
      "2025-04-03,13:17:43 | INFO |   imagenet_v2: None\r\n",
      "2025-04-03,13:17:43 | INFO |   imagenet_val: None\r\n",
      "2025-04-03,13:17:43 | INFO |   lambda_dino: 0.25\r\n",
      "2025-04-03,13:17:43 | INFO |   local_loss: False\r\n",
      "2025-04-03,13:17:43 | INFO |   local_rank: 0\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_image: False\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_image_freeze_bn_stats: False\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_image_unlocked_groups: 0\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_text: False\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_text_freeze_layer_norm: False\r\n",
      "2025-04-03,13:17:43 | INFO |   lock_text_unlocked_layers: 0\r\n",
      "2025-04-03,13:17:43 | INFO |   log_every_n_steps: 100\r\n",
      "2025-04-03,13:17:43 | INFO |   log_level: 20\r\n",
      "2025-04-03,13:17:43 | INFO |   log_local: False\r\n",
      "2025-04-03,13:17:43 | INFO |   log_path: checkpoints/dino_proj_fine_tuned_Clip/out.log\r\n",
      "2025-04-03,13:17:43 | INFO |   logs: checkpoints\r\n",
      "2025-04-03,13:17:43 | INFO |   loss_dist_impl: None\r\n",
      "2025-04-03,13:17:43 | INFO |   lr: 5e-06\r\n",
      "2025-04-03,13:17:43 | INFO |   lr_cooldown_end: 0.0\r\n",
      "2025-04-03,13:17:43 | INFO |   lr_cooldown_power: 1.0\r\n",
      "2025-04-03,13:17:43 | INFO |   lr_scheduler: cosine\r\n",
      "2025-04-03,13:17:43 | INFO |   model: RN50\r\n",
      "2025-04-03,13:17:43 | INFO |   momentum: None\r\n",
      "2025-04-03,13:17:43 | INFO |   name: dino_proj_fine_tuned_Clip\r\n",
      "2025-04-03,13:17:43 | INFO |   no_set_device_rank: False\r\n",
      "2025-04-03,13:17:43 | INFO |   opt: adamw\r\n",
      "2025-04-03,13:17:43 | INFO |   precision: amp\r\n",
      "2025-04-03,13:17:43 | INFO |   pretrained: /kaggle/working/clip_final.pt\r\n",
      "2025-04-03,13:17:43 | INFO |   pretrained_image: False\r\n",
      "2025-04-03,13:17:43 | INFO |   rank: 0\r\n",
      "2025-04-03,13:17:43 | INFO |   remote_sync: None\r\n",
      "2025-04-03,13:17:43 | INFO |   remote_sync_frequency: 300\r\n",
      "2025-04-03,13:17:43 | INFO |   remote_sync_protocol: s3\r\n",
      "2025-04-03,13:17:43 | INFO |   report_to: \r\n",
      "2025-04-03,13:17:43 | INFO |   resume: None\r\n",
      "2025-04-03,13:17:43 | INFO |   save_frequency: 1\r\n",
      "2025-04-03,13:17:43 | INFO |   save_most_recent: False\r\n",
      "2025-04-03,13:17:43 | INFO |   seed: 42\r\n",
      "2025-04-03,13:17:43 | INFO |   siglip: False\r\n",
      "2025-04-03,13:17:43 | INFO |   skip_scheduler: False\r\n",
      "2025-04-03,13:17:43 | INFO |   soft_temprature: 0.02\r\n",
      "2025-04-03,13:17:43 | INFO |   tensorboard: False\r\n",
      "2025-04-03,13:17:43 | INFO |   tensorboard_path: \r\n",
      "2025-04-03,13:17:43 | INFO |   torchcompile: False\r\n",
      "2025-04-03,13:17:43 | INFO |   torchscript: False\r\n",
      "2025-04-03,13:17:43 | INFO |   trace: False\r\n",
      "2025-04-03,13:17:43 | INFO |   train_data: /kaggle/working/train_data_karpathy.csv\r\n",
      "2025-04-03,13:17:43 | INFO |   train_data_upsampling_factors: None\r\n",
      "2025-04-03,13:17:43 | INFO |   train_num_samples: None\r\n",
      "2025-04-03,13:17:43 | INFO |   use_bn_sync: False\r\n",
      "2025-04-03,13:17:43 | INFO |   use_bnb_linear: None\r\n",
      "2025-04-03,13:17:43 | INFO |   use_dino_reg: True\r\n",
      "2025-04-03,13:17:43 | INFO |   use_soft_labels: False\r\n",
      "2025-04-03,13:17:43 | INFO |   val_data: None\r\n",
      "2025-04-03,13:17:43 | INFO |   val_frequency: 1\r\n",
      "2025-04-03,13:17:43 | INFO |   val_num_samples: None\r\n",
      "2025-04-03,13:17:43 | INFO |   wandb: False\r\n",
      "2025-04-03,13:17:43 | INFO |   wandb_notes: \r\n",
      "2025-04-03,13:17:43 | INFO |   wandb_project_name: open-clip\r\n",
      "2025-04-03,13:17:43 | INFO |   warmup: 1000\r\n",
      "2025-04-03,13:17:43 | INFO |   wd: 0.2\r\n",
      "2025-04-03,13:17:43 | INFO |   workers: 4\r\n",
      "2025-04-03,13:17:43 | INFO |   world_size: 1\r\n",
      "2025-04-03,13:17:43 | INFO |   zeroshot_frequency: 2\r\n",
      "2025-04-03,13:17:43 | INFO | Created AdamW (adamw) optimizer: lr: 5e-06, betas: (0.9, 0.999), eps: 1e-08, weight_decay: 0.2, amsgrad: False, foreach: None, maximize: False, capturable: False, differentiable: False, fused: None\r\n",
      "2025-04-03,13:17:44 | INFO | Start epoch 0\r\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\r\n",
      "2025-04-03,13:17:52 | INFO | Train Epoch: 0 [    48/145000 (0%)] Data (t): 1.303 Batch (t): 8.530, 5.62747/s, 5.62747/s/gpu LR: 0.000000 Logit Scale: 73.816 Original_clip_loss: 1.7160 (1.7160) Loss: 1.7160 (1.7160) Dino_reguarizing_loss: 0.10208 (0.10208)\r\n",
      "2025-04-03,13:19:33 | INFO | Train Epoch: 0 [  4848/145000 (3%)] Data (t): 0.000 Batch (t): 1.005, 47.5769/s, 47.5769/s/gpu LR: 0.000001 Logit Scale: 73.814 Original_clip_loss: 1.6891 (1.7025) Loss: 1.6891 (1.7025) Dino_reguarizing_loss: 0.11369 (0.10789)\r\n",
      "2025-04-03,13:21:19 | INFO | Train Epoch: 0 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.062, 43.8781/s, 43.8781/s/gpu LR: 0.000001 Logit Scale: 73.809 Original_clip_loss: 1.5689 (1.6580) Loss: 1.5689 (1.6580) Dino_reguarizing_loss: 0.10424 (0.10667)\r\n",
      "2025-04-03,13:23:06 | INFO | Train Epoch: 0 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.074, 44.0755/s, 44.0755/s/gpu LR: 0.000002 Logit Scale: 73.801 Original_clip_loss: 1.5726 (1.6366) Loss: 1.5726 (1.6366) Dino_reguarizing_loss: 0.090089 (0.10253)\r\n",
      "2025-04-03,13:24:53 | INFO | Train Epoch: 0 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.072, 45.0997/s, 45.0997/s/gpu LR: 0.000002 Logit Scale: 73.790 Original_clip_loss: 1.8540 (1.6801) Loss: 1.8540 (1.6801) Dino_reguarizing_loss: 0.096577 (0.10134)\r\n",
      "2025-04-03,13:26:41 | INFO | Train Epoch: 0 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.072, 41.8627/s, 41.8627/s/gpu LR: 0.000003 Logit Scale: 73.776 Original_clip_loss: 1.6673 (1.6780) Loss: 1.6673 (1.6780) Dino_reguarizing_loss: 0.10207 (0.10146)\r\n",
      "2025-04-03,13:28:28 | INFO | Train Epoch: 0 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.071, 42.0632/s, 42.0632/s/gpu LR: 0.000003 Logit Scale: 73.760 Original_clip_loss: 1.4007 (1.6384) Loss: 1.4007 (1.6384) Dino_reguarizing_loss: 0.075187 (0.097704)\r\n",
      "2025-04-03,13:30:15 | INFO | Train Epoch: 0 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.072, 47.2424/s, 47.2424/s/gpu LR: 0.000004 Logit Scale: 73.742 Original_clip_loss: 1.0127 (1.5602) Loss: 1.0127 (1.5602) Dino_reguarizing_loss: 0.080409 (0.095542)\r\n",
      "2025-04-03,13:32:01 | INFO | Train Epoch: 0 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.064, 42.4280/s, 42.4280/s/gpu LR: 0.000004 Logit Scale: 73.723 Original_clip_loss: 1.0969 (1.5087) Loss: 1.0969 (1.5087) Dino_reguarizing_loss: 0.066095 (0.092271)\r\n",
      "2025-04-03,13:33:49 | INFO | Train Epoch: 0 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.078, 42.6708/s, 42.6708/s/gpu LR: 0.000005 Logit Scale: 73.703 Original_clip_loss: 1.2753 (1.4853) Loss: 1.2753 (1.4853) Dino_reguarizing_loss: 0.067633 (0.089807)\r\n",
      "2025-04-03,13:35:36 | INFO | Train Epoch: 0 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.069, 42.6319/s, 42.6319/s/gpu LR: 0.000005 Logit Scale: 73.682 Original_clip_loss: 0.82335 (1.4252) Loss: 0.82335 (1.4252) Dino_reguarizing_loss: 0.054362 (0.086585)\r\n",
      "2025-04-03,13:37:23 | INFO | Train Epoch: 0 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.072, 42.8774/s, 42.8774/s/gpu LR: 0.000005 Logit Scale: 73.662 Original_clip_loss: 1.0371 (1.3928) Loss: 1.0371 (1.3928) Dino_reguarizing_loss: 0.051197 (0.083636)\r\n",
      "2025-04-03,13:39:11 | INFO | Train Epoch: 0 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.077, 47.5512/s, 47.5512/s/gpu LR: 0.000005 Logit Scale: 73.646 Original_clip_loss: 0.76305 (1.3444) Loss: 0.76305 (1.3444) Dino_reguarizing_loss: 0.052779 (0.081262)\r\n",
      "2025-04-03,13:40:58 | INFO | Train Epoch: 0 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.070, 46.6457/s, 46.6457/s/gpu LR: 0.000005 Logit Scale: 73.631 Original_clip_loss: 0.78308 (1.3043) Loss: 0.78308 (1.3043) Dino_reguarizing_loss: 0.044405 (0.078629)\r\n",
      "2025-04-03,13:42:46 | INFO | Train Epoch: 0 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.082, 45.7473/s, 45.7473/s/gpu LR: 0.000005 Logit Scale: 73.616 Original_clip_loss: 0.81613 (1.2717) Loss: 0.81613 (1.2717) Dino_reguarizing_loss: 0.049657 (0.076698)\r\n",
      "2025-04-03,13:44:35 | INFO | Train Epoch: 0 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.091, 41.2026/s, 41.2026/s/gpu LR: 0.000005 Logit Scale: 73.604 Original_clip_loss: 0.83201 (1.2443) Loss: 0.83201 (1.2443) Dino_reguarizing_loss: 0.044308 (0.074674)\r\n",
      "2025-04-03,13:46:24 | INFO | Train Epoch: 0 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.085, 46.7237/s, 46.7237/s/gpu LR: 0.000005 Logit Scale: 73.592 Original_clip_loss: 0.83488 (1.2202) Loss: 0.83488 (1.2202) Dino_reguarizing_loss: 0.034436 (0.072307)\r\n",
      "2025-04-03,13:48:12 | INFO | Train Epoch: 0 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.082, 44.7482/s, 44.7482/s/gpu LR: 0.000005 Logit Scale: 73.581 Original_clip_loss: 0.89234 (1.2020) Loss: 0.89234 (1.2020) Dino_reguarizing_loss: 0.030788 (0.070000)\r\n",
      "2025-04-03,13:50:01 | INFO | Train Epoch: 0 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.085, 40.1463/s, 40.1463/s/gpu LR: 0.000005 Logit Scale: 73.570 Original_clip_loss: 0.84507 (1.1832) Loss: 0.84507 (1.1832) Dino_reguarizing_loss: 0.038352 (0.068334)\r\n",
      "2025-04-03,13:51:49 | INFO | Train Epoch: 0 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.088, 46.5851/s, 46.5851/s/gpu LR: 0.000005 Logit Scale: 73.560 Original_clip_loss: 0.39885 (1.1440) Loss: 0.39885 (1.1440) Dino_reguarizing_loss: 0.042868 (0.067061)\r\n",
      "2025-04-03,13:53:40 | INFO | Train Epoch: 0 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.102, 46.8422/s, 46.8422/s/gpu LR: 0.000005 Logit Scale: 73.552 Original_clip_loss: 0.56806 (1.1165) Loss: 0.56806 (1.1165) Dino_reguarizing_loss: 0.032120 (0.065397)\r\n",
      "2025-04-03,13:55:30 | INFO | Train Epoch: 0 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.107, 38.9211/s, 38.9211/s/gpu LR: 0.000005 Logit Scale: 73.544 Original_clip_loss: 0.67887 (1.0966) Loss: 0.67887 (1.0966) Dino_reguarizing_loss: 0.032548 (0.063904)\r\n",
      "2025-04-03,13:57:21 | INFO | Train Epoch: 0 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.107, 41.7560/s, 41.7560/s/gpu LR: 0.000005 Logit Scale: 73.536 Original_clip_loss: 0.61008 (1.0755) Loss: 0.61008 (1.0755) Dino_reguarizing_loss: 0.030072 (0.062433)\r\n",
      "2025-04-03,13:59:12 | INFO | Train Epoch: 0 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.112, 45.8813/s, 45.8813/s/gpu LR: 0.000005 Logit Scale: 73.528 Original_clip_loss: 0.63426 (1.0571) Loss: 0.63426 (1.0571) Dino_reguarizing_loss: 0.025990 (0.060915)\r\n",
      "2025-04-03,14:01:00 | INFO | Train Epoch: 0 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.080, 46.6430/s, 46.6430/s/gpu LR: 0.000005 Logit Scale: 73.521 Original_clip_loss: 0.80770 (1.0471) Loss: 0.80770 (1.0471) Dino_reguarizing_loss: 0.025598 (0.059502)\r\n",
      "2025-04-03,14:02:49 | INFO | Train Epoch: 0 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.091, 41.9437/s, 41.9437/s/gpu LR: 0.000005 Logit Scale: 73.516 Original_clip_loss: 0.53219 (1.0273) Loss: 0.53219 (1.0273) Dino_reguarizing_loss: 0.027474 (0.058270)\r\n",
      "2025-04-03,14:04:38 | INFO | Train Epoch: 0 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.090, 38.4166/s, 38.4166/s/gpu LR: 0.000005 Logit Scale: 73.511 Original_clip_loss: 0.75870 (1.0174) Loss: 0.75870 (1.0174) Dino_reguarizing_loss: 0.019611 (0.056838)\r\n",
      "2025-04-03,14:06:29 | INFO | Train Epoch: 0 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.105, 41.8166/s, 41.8166/s/gpu LR: 0.000005 Logit Scale: 73.504 Original_clip_loss: 0.59265 (1.0022) Loss: 0.59265 (1.0022) Dino_reguarizing_loss: 0.021978 (0.055593)\r\n",
      "2025-04-03,14:08:19 | INFO | Train Epoch: 0 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.101, 41.2107/s, 41.2107/s/gpu LR: 0.000005 Logit Scale: 73.501 Original_clip_loss: 0.90448 (0.99884) Loss: 0.90448 (0.99884) Dino_reguarizing_loss: 0.020444 (0.054381)\r\n",
      "2025-04-03,14:10:09 | INFO | Train Epoch: 0 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.101, 44.1680/s, 44.1680/s/gpu LR: 0.000005 Logit Scale: 73.495 Original_clip_loss: 0.67481 (0.98804) Loss: 0.67481 (0.98804) Dino_reguarizing_loss: 0.024621 (0.053389)\r\n",
      "2025-04-03,14:12:00 | INFO | Train Epoch: 0 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.113, 45.8040/s, 45.8040/s/gpu LR: 0.000005 Logit Scale: 73.491 Original_clip_loss: 0.54293 (0.97368) Loss: 0.54293 (0.97368) Dino_reguarizing_loss: 0.020593 (0.052331)\r\n",
      "2025-04-03,14:12:20 | INFO | Train Epoch: 0 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.065, 47.6082/s, 47.6082/s/gpu LR: 0.000005 Logit Scale: 73.491 Original_clip_loss: 0.54284 (0.96022) Loss: 0.54284 (0.96022) Dino_reguarizing_loss: 0.022781 (0.051408)\r\n",
      "2025-04-03,14:12:22 | INFO | Start epoch 1\r\n",
      "2025-04-03,14:12:25 | INFO | Train Epoch: 1 [    48/145000 (0%)] Data (t): 0.988 Batch (t): 2.307, 20.8021/s, 20.8021/s/gpu LR: 0.000005 Logit Scale: 73.491 Original_clip_loss: 0.59782 (0.59782) Loss: 0.59782 (0.59782) Dino_reguarizing_loss: 0.022669 (0.022669)\r\n",
      "2025-04-03,14:14:15 | INFO | Train Epoch: 1 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.104, 45.9615/s, 45.9615/s/gpu LR: 0.000005 Logit Scale: 73.491 Original_clip_loss: 0.57818 (0.58800) Loss: 0.57818 (0.58800) Dino_reguarizing_loss: 0.023132 (0.022901)\r\n",
      "2025-04-03,14:16:06 | INFO | Train Epoch: 1 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.104, 38.9451/s, 38.9451/s/gpu LR: 0.000005 Logit Scale: 73.489 Original_clip_loss: 0.63955 (0.60518) Loss: 0.63955 (0.60518) Dino_reguarizing_loss: 0.024733 (0.023512)\r\n",
      "2025-04-03,14:17:54 | INFO | Train Epoch: 1 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.087, 39.6220/s, 39.6220/s/gpu LR: 0.000005 Logit Scale: 73.488 Original_clip_loss: 0.53674 (0.58807) Loss: 0.53674 (0.58807) Dino_reguarizing_loss: 0.024530 (0.023766)\r\n",
      "2025-04-03,14:19:44 | INFO | Train Epoch: 1 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.101, 39.3122/s, 39.3122/s/gpu LR: 0.000005 Logit Scale: 73.486 Original_clip_loss: 0.48878 (0.56821) Loss: 0.48878 (0.56821) Dino_reguarizing_loss: 0.017799 (0.022573)\r\n",
      "2025-04-03,14:21:34 | INFO | Train Epoch: 1 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.097, 40.1908/s, 40.1908/s/gpu LR: 0.000005 Logit Scale: 73.483 Original_clip_loss: 0.45859 (0.54994) Loss: 0.45859 (0.54994) Dino_reguarizing_loss: 0.019305 (0.022028)\r\n",
      "2025-04-03,14:23:22 | INFO | Train Epoch: 1 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.084, 44.1057/s, 44.1057/s/gpu LR: 0.000005 Logit Scale: 73.483 Original_clip_loss: 0.52064 (0.54576) Loss: 0.52064 (0.54576) Dino_reguarizing_loss: 0.023279 (0.022207)\r\n",
      "2025-04-03,14:25:12 | INFO | Train Epoch: 1 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.101, 43.3758/s, 43.3758/s/gpu LR: 0.000005 Logit Scale: 73.479 Original_clip_loss: 0.62413 (0.55555) Loss: 0.62413 (0.55555) Dino_reguarizing_loss: 0.021717 (0.022146)\r\n",
      "2025-04-03,14:27:02 | INFO | Train Epoch: 1 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.094, 43.3739/s, 43.3739/s/gpu LR: 0.000005 Logit Scale: 73.478 Original_clip_loss: 0.36666 (0.53457) Loss: 0.36666 (0.53457) Dino_reguarizing_loss: 0.022257 (0.022158)\r\n",
      "2025-04-03,14:28:50 | INFO | Train Epoch: 1 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.081, 45.6705/s, 45.6705/s/gpu LR: 0.000005 Logit Scale: 73.479 Original_clip_loss: 0.46249 (0.52736) Loss: 0.46249 (0.52736) Dino_reguarizing_loss: 0.021633 (0.022105)\r\n",
      "2025-04-03,14:30:40 | INFO | Train Epoch: 1 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.104, 44.1407/s, 44.1407/s/gpu LR: 0.000005 Logit Scale: 73.478 Original_clip_loss: 0.33127 (0.50953) Loss: 0.33127 (0.50953) Dino_reguarizing_loss: 0.018146 (0.021746)\r\n",
      "2025-04-03,14:32:29 | INFO | Train Epoch: 1 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.082, 42.6850/s, 42.6850/s/gpu LR: 0.000005 Logit Scale: 73.473 Original_clip_loss: 0.50422 (0.50909) Loss: 0.50422 (0.50909) Dino_reguarizing_loss: 0.028319 (0.022293)\r\n",
      "2025-04-03,14:34:18 | INFO | Train Epoch: 1 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.098, 46.1756/s, 46.1756/s/gpu LR: 0.000005 Logit Scale: 73.471 Original_clip_loss: 0.35872 (0.49752) Loss: 0.35872 (0.49752) Dino_reguarizing_loss: 0.020255 (0.022136)\r\n",
      "2025-04-03,14:36:06 | INFO | Train Epoch: 1 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.080, 44.7972/s, 44.7972/s/gpu LR: 0.000005 Logit Scale: 73.470 Original_clip_loss: 0.42462 (0.49232) Loss: 0.42462 (0.49232) Dino_reguarizing_loss: 0.020014 (0.021985)\r\n",
      "2025-04-03,14:37:56 | INFO | Train Epoch: 1 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.093, 41.8390/s, 41.8390/s/gpu LR: 0.000005 Logit Scale: 73.464 Original_clip_loss: 0.38037 (0.48485) Loss: 0.38037 (0.48485) Dino_reguarizing_loss: 0.020037 (0.021855)\r\n",
      "2025-04-03,14:39:46 | INFO | Train Epoch: 1 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.104, 40.4076/s, 40.4076/s/gpu LR: 0.000005 Logit Scale: 73.463 Original_clip_loss: 0.61436 (0.49295) Loss: 0.61436 (0.49295) Dino_reguarizing_loss: 0.020649 (0.021780)\r\n",
      "2025-04-03,14:41:35 | INFO | Train Epoch: 1 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.086, 46.2827/s, 46.2827/s/gpu LR: 0.000005 Logit Scale: 73.464 Original_clip_loss: 0.47615 (0.49196) Loss: 0.47615 (0.49196) Dino_reguarizing_loss: 0.019464 (0.021643)\r\n",
      "2025-04-03,14:43:23 | INFO | Train Epoch: 1 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.085, 42.5395/s, 42.5395/s/gpu LR: 0.000005 Logit Scale: 73.464 Original_clip_loss: 0.23351 (0.47760) Loss: 0.23351 (0.47760) Dino_reguarizing_loss: 0.019325 (0.021515)\r\n",
      "2025-04-03,14:45:12 | INFO | Train Epoch: 1 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.086, 45.7026/s, 45.7026/s/gpu LR: 0.000005 Logit Scale: 73.465 Original_clip_loss: 0.22093 (0.46409) Loss: 0.22093 (0.46409) Dino_reguarizing_loss: 0.016463 (0.021249)\r\n",
      "2025-04-03,14:47:01 | INFO | Train Epoch: 1 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.092, 41.7330/s, 41.7330/s/gpu LR: 0.000005 Logit Scale: 73.462 Original_clip_loss: 0.37976 (0.45987) Loss: 0.37976 (0.45987) Dino_reguarizing_loss: 0.020295 (0.021201)\r\n",
      "2025-04-03,14:48:50 | INFO | Train Epoch: 1 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.086, 40.4067/s, 40.4067/s/gpu LR: 0.000005 Logit Scale: 73.462 Original_clip_loss: 0.45182 (0.45949) Loss: 0.45182 (0.45949) Dino_reguarizing_loss: 0.023495 (0.021310)\r\n",
      "2025-04-03,14:50:39 | INFO | Train Epoch: 1 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.095, 47.7544/s, 47.7544/s/gpu LR: 0.000005 Logit Scale: 73.462 Original_clip_loss: 0.29650 (0.45208) Loss: 0.29650 (0.45208) Dino_reguarizing_loss: 0.022516 (0.021365)\r\n",
      "2025-04-03,14:52:27 | INFO | Train Epoch: 1 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.077, 45.6888/s, 45.6888/s/gpu LR: 0.000005 Logit Scale: 73.460 Original_clip_loss: 0.32868 (0.44672) Loss: 0.32868 (0.44672) Dino_reguarizing_loss: 0.017147 (0.021182)\r\n",
      "2025-04-03,14:54:18 | INFO | Train Epoch: 1 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.108, 44.3169/s, 44.3169/s/gpu LR: 0.000005 Logit Scale: 73.458 Original_clip_loss: 0.31822 (0.44136) Loss: 0.31822 (0.44136) Dino_reguarizing_loss: 0.019500 (0.021112)\r\n",
      "2025-04-03,14:56:07 | INFO | Train Epoch: 1 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.097, 39.0966/s, 39.0966/s/gpu LR: 0.000005 Logit Scale: 73.454 Original_clip_loss: 0.33738 (0.43720) Loss: 0.33738 (0.43720) Dino_reguarizing_loss: 0.019566 (0.021050)\r\n",
      "2025-04-03,14:57:57 | INFO | Train Epoch: 1 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.093, 42.4728/s, 42.4728/s/gpu LR: 0.000005 Logit Scale: 73.454 Original_clip_loss: 0.52521 (0.44059) Loss: 0.52521 (0.44059) Dino_reguarizing_loss: 0.018217 (0.020941)\r\n",
      "2025-04-03,14:59:46 | INFO | Train Epoch: 1 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.095, 47.2544/s, 47.2544/s/gpu LR: 0.000005 Logit Scale: 73.456 Original_clip_loss: 0.47160 (0.44174) Loss: 0.47160 (0.44174) Dino_reguarizing_loss: 0.015373 (0.020735)\r\n",
      "2025-04-03,15:01:36 | INFO | Train Epoch: 1 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.103, 40.9510/s, 40.9510/s/gpu LR: 0.000005 Logit Scale: 73.458 Original_clip_loss: 0.47301 (0.44285) Loss: 0.47301 (0.44285) Dino_reguarizing_loss: 0.018782 (0.020665)\r\n",
      "2025-04-03,15:03:24 | INFO | Train Epoch: 1 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.080, 44.9925/s, 44.9925/s/gpu LR: 0.000005 Logit Scale: 73.451 Original_clip_loss: 0.45161 (0.44316) Loss: 0.45161 (0.44316) Dino_reguarizing_loss: 0.023683 (0.020769)\r\n",
      "2025-04-03,15:05:13 | INFO | Train Epoch: 1 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.086, 47.6653/s, 47.6653/s/gpu LR: 0.000005 Logit Scale: 73.453 Original_clip_loss: 0.55004 (0.44672) Loss: 0.55004 (0.44672) Dino_reguarizing_loss: 0.017148 (0.020648)\r\n",
      "2025-04-03,15:07:02 | INFO | Train Epoch: 1 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.090, 46.4171/s, 46.4171/s/gpu LR: 0.000005 Logit Scale: 73.455 Original_clip_loss: 0.30893 (0.44227) Loss: 0.30893 (0.44227) Dino_reguarizing_loss: 0.019136 (0.020599)\r\n",
      "2025-04-03,15:07:22 | INFO | Train Epoch: 1 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.050, 47.5044/s, 47.5044/s/gpu LR: 0.000005 Logit Scale: 73.455 Original_clip_loss: 0.30128 (0.43787) Loss: 0.30128 (0.43787) Dino_reguarizing_loss: 0.020179 (0.020586)\r\n",
      "2025-04-03,15:07:24 | INFO | Start epoch 2\r\n",
      "2025-04-03,15:07:26 | INFO | Train Epoch: 2 [    48/145000 (0%)] Data (t): 0.922 Batch (t): 2.342, 20.4968/s, 20.4968/s/gpu LR: 0.000005 Logit Scale: 73.455 Original_clip_loss: 0.27099 (0.27099) Loss: 0.27099 (0.27099) Dino_reguarizing_loss: 0.020659 (0.020659)\r\n",
      "2025-04-03,15:09:16 | INFO | Train Epoch: 2 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.101, 42.6015/s, 42.6015/s/gpu LR: 0.000005 Logit Scale: 73.466 Original_clip_loss: 0.31533 (0.29316) Loss: 0.31533 (0.29316) Dino_reguarizing_loss: 0.023847 (0.022253)\r\n",
      "2025-04-03,15:11:06 | INFO | Train Epoch: 2 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.092, 46.7609/s, 46.7609/s/gpu LR: 0.000005 Logit Scale: 73.476 Original_clip_loss: 0.33336 (0.30656) Loss: 0.33336 (0.30656) Dino_reguarizing_loss: 0.021393 (0.021966)\r\n",
      "2025-04-03,15:12:54 | INFO | Train Epoch: 2 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.085, 40.3431/s, 40.3431/s/gpu LR: 0.000005 Logit Scale: 73.483 Original_clip_loss: 0.40135 (0.33026) Loss: 0.40135 (0.33026) Dino_reguarizing_loss: 0.016938 (0.020709)\r\n",
      "2025-04-03,15:14:42 | INFO | Train Epoch: 2 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.081, 46.7021/s, 46.7021/s/gpu LR: 0.000005 Logit Scale: 73.489 Original_clip_loss: 0.48008 (0.36022) Loss: 0.48008 (0.36022) Dino_reguarizing_loss: 0.021752 (0.020918)\r\n",
      "2025-04-03,15:16:33 | INFO | Train Epoch: 2 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.106, 46.5506/s, 46.5506/s/gpu LR: 0.000005 Logit Scale: 73.490 Original_clip_loss: 0.32951 (0.35510) Loss: 0.32951 (0.35510) Dino_reguarizing_loss: 0.018230 (0.020470)\r\n",
      "2025-04-03,15:18:23 | INFO | Train Epoch: 2 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.097, 43.9203/s, 43.9203/s/gpu LR: 0.000005 Logit Scale: 73.488 Original_clip_loss: 0.41782 (0.36406) Loss: 0.41782 (0.36406) Dino_reguarizing_loss: 0.018139 (0.020137)\r\n",
      "2025-04-03,15:20:13 | INFO | Train Epoch: 2 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.099, 44.7682/s, 44.7682/s/gpu LR: 0.000005 Logit Scale: 73.494 Original_clip_loss: 0.30654 (0.35687) Loss: 0.30654 (0.35687) Dino_reguarizing_loss: 0.016537 (0.019687)\r\n",
      "2025-04-03,15:22:01 | INFO | Train Epoch: 2 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.089, 45.0738/s, 45.0738/s/gpu LR: 0.000005 Logit Scale: 73.495 Original_clip_loss: 0.58008 (0.38167) Loss: 0.58008 (0.38167) Dino_reguarizing_loss: 0.016319 (0.019313)\r\n",
      "2025-04-03,15:23:51 | INFO | Train Epoch: 2 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.097, 41.9245/s, 41.9245/s/gpu LR: 0.000005 Logit Scale: 73.498 Original_clip_loss: 0.54400 (0.39791) Loss: 0.54400 (0.39791) Dino_reguarizing_loss: 0.016145 (0.018996)\r\n",
      "2025-04-03,15:25:42 | INFO | Train Epoch: 2 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.105, 44.1420/s, 44.1420/s/gpu LR: 0.000004 Logit Scale: 73.506 Original_clip_loss: 0.33532 (0.39222) Loss: 0.33532 (0.39222) Dino_reguarizing_loss: 0.024149 (0.019464)\r\n",
      "2025-04-03,15:27:31 | INFO | Train Epoch: 2 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.096, 45.7885/s, 45.7885/s/gpu LR: 0.000004 Logit Scale: 73.512 Original_clip_loss: 0.37807 (0.39104) Loss: 0.37807 (0.39104) Dino_reguarizing_loss: 0.020359 (0.019539)\r\n",
      "2025-04-03,15:29:18 | INFO | Train Epoch: 2 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.072, 44.8136/s, 44.8136/s/gpu LR: 0.000004 Logit Scale: 73.520 Original_clip_loss: 0.71688 (0.41610) Loss: 0.71688 (0.41610) Dino_reguarizing_loss: 0.020237 (0.019593)\r\n",
      "2025-04-03,15:31:08 | INFO | Train Epoch: 2 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.097, 37.8220/s, 37.8220/s/gpu LR: 0.000004 Logit Scale: 73.522 Original_clip_loss: 0.15090 (0.39716) Loss: 0.15090 (0.39716) Dino_reguarizing_loss: 0.022212 (0.019780)\r\n",
      "2025-04-03,15:32:59 | INFO | Train Epoch: 2 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.106, 46.7640/s, 46.7640/s/gpu LR: 0.000004 Logit Scale: 73.525 Original_clip_loss: 0.24661 (0.38712) Loss: 0.24661 (0.38712) Dino_reguarizing_loss: 0.017996 (0.019661)\r\n",
      "2025-04-03,15:34:48 | INFO | Train Epoch: 2 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.094, 39.7043/s, 39.7043/s/gpu LR: 0.000004 Logit Scale: 73.525 Original_clip_loss: 0.39949 (0.38790) Loss: 0.39949 (0.38790) Dino_reguarizing_loss: 0.019915 (0.019677)\r\n",
      "2025-04-03,15:36:36 | INFO | Train Epoch: 2 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.080, 43.2552/s, 43.2552/s/gpu LR: 0.000004 Logit Scale: 73.530 Original_clip_loss: 0.53423 (0.39650) Loss: 0.53423 (0.39650) Dino_reguarizing_loss: 0.018095 (0.019584)\r\n",
      "2025-04-03,15:38:26 | INFO | Train Epoch: 2 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.098, 46.6885/s, 46.6885/s/gpu LR: 0.000004 Logit Scale: 73.531 Original_clip_loss: 0.26084 (0.38897) Loss: 0.26084 (0.38897) Dino_reguarizing_loss: 0.017975 (0.019494)\r\n",
      "2025-04-03,15:40:15 | INFO | Train Epoch: 2 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.097, 44.9404/s, 44.9404/s/gpu LR: 0.000004 Logit Scale: 73.530 Original_clip_loss: 0.45657 (0.39252) Loss: 0.45657 (0.39252) Dino_reguarizing_loss: 0.019031 (0.019470)\r\n",
      "2025-04-03,15:42:04 | INFO | Train Epoch: 2 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.082, 46.9820/s, 46.9820/s/gpu LR: 0.000004 Logit Scale: 73.532 Original_clip_loss: 0.37128 (0.39146) Loss: 0.37128 (0.39146) Dino_reguarizing_loss: 0.021271 (0.019560)\r\n",
      "2025-04-03,15:43:51 | INFO | Train Epoch: 2 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.076, 41.7874/s, 41.7874/s/gpu LR: 0.000004 Logit Scale: 73.530 Original_clip_loss: 0.27873 (0.38609) Loss: 0.27873 (0.38609) Dino_reguarizing_loss: 0.016586 (0.019418)\r\n",
      "2025-04-03,15:45:42 | INFO | Train Epoch: 2 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.106, 45.8467/s, 45.8467/s/gpu LR: 0.000004 Logit Scale: 73.531 Original_clip_loss: 0.58763 (0.39525) Loss: 0.58763 (0.39525) Dino_reguarizing_loss: 0.013483 (0.019149)\r\n",
      "2025-04-03,15:47:31 | INFO | Train Epoch: 2 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.091, 46.6972/s, 46.6972/s/gpu LR: 0.000004 Logit Scale: 73.534 Original_clip_loss: 0.47910 (0.39890) Loss: 0.47910 (0.39890) Dino_reguarizing_loss: 0.018014 (0.019099)\r\n",
      "2025-04-03,15:49:20 | INFO | Train Epoch: 2 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.089, 47.6468/s, 47.6468/s/gpu LR: 0.000004 Logit Scale: 73.534 Original_clip_loss: 0.31174 (0.39527) Loss: 0.31174 (0.39527) Dino_reguarizing_loss: 0.017098 (0.019016)\r\n",
      "2025-04-03,15:51:09 | INFO | Train Epoch: 2 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.090, 44.7765/s, 44.7765/s/gpu LR: 0.000004 Logit Scale: 73.540 Original_clip_loss: 0.21879 (0.38821) Loss: 0.21879 (0.38821) Dino_reguarizing_loss: 0.015699 (0.018883)\r\n",
      "2025-04-03,15:52:57 | INFO | Train Epoch: 2 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.082, 39.7238/s, 39.7238/s/gpu LR: 0.000004 Logit Scale: 73.539 Original_clip_loss: 0.41241 (0.38914) Loss: 0.41241 (0.38914) Dino_reguarizing_loss: 0.015501 (0.018753)\r\n",
      "2025-04-03,15:54:46 | INFO | Train Epoch: 2 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.089, 43.4969/s, 43.4969/s/gpu LR: 0.000004 Logit Scale: 73.544 Original_clip_loss: 0.72350 (0.40152) Loss: 0.72350 (0.40152) Dino_reguarizing_loss: 0.023720 (0.018937)\r\n",
      "2025-04-03,15:56:35 | INFO | Train Epoch: 2 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.088, 44.8111/s, 44.8111/s/gpu LR: 0.000004 Logit Scale: 73.542 Original_clip_loss: 0.29330 (0.39766) Loss: 0.29330 (0.39766) Dino_reguarizing_loss: 0.016998 (0.018868)\r\n",
      "2025-04-03,15:58:23 | INFO | Train Epoch: 2 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.084, 39.0790/s, 39.0790/s/gpu LR: 0.000004 Logit Scale: 73.547 Original_clip_loss: 0.10270 (0.38749) Loss: 0.10270 (0.38749) Dino_reguarizing_loss: 0.020560 (0.018926)\r\n",
      "2025-04-03,16:00:11 | INFO | Train Epoch: 2 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.077, 39.7962/s, 39.7962/s/gpu LR: 0.000004 Logit Scale: 73.553 Original_clip_loss: 0.23017 (0.38224) Loss: 0.23017 (0.38224) Dino_reguarizing_loss: 0.013384 (0.018741)\r\n",
      "2025-04-03,16:01:59 | INFO | Train Epoch: 2 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.078, 44.7695/s, 44.7695/s/gpu LR: 0.000004 Logit Scale: 73.552 Original_clip_loss: 0.28741 (0.37918) Loss: 0.28741 (0.37918) Dino_reguarizing_loss: 0.014445 (0.018603)\r\n",
      "2025-04-03,16:02:19 | INFO | Train Epoch: 2 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.041, 48.5188/s, 48.5188/s/gpu LR: 0.000004 Logit Scale: 73.551 Original_clip_loss: 0.31162 (0.37707) Loss: 0.31162 (0.37707) Dino_reguarizing_loss: 0.017628 (0.018572)\r\n",
      "2025-04-03,16:02:21 | INFO | Start epoch 3\r\n",
      "2025-04-03,16:02:23 | INFO | Train Epoch: 3 [    48/145000 (0%)] Data (t): 0.970 Batch (t): 2.330, 20.5974/s, 20.5974/s/gpu LR: 0.000004 Logit Scale: 73.551 Original_clip_loss: 0.20195 (0.20195) Loss: 0.20195 (0.20195) Dino_reguarizing_loss: 0.019654 (0.019654)\r\n",
      "2025-04-03,16:04:12 | INFO | Train Epoch: 3 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.091, 46.6131/s, 46.6131/s/gpu LR: 0.000004 Logit Scale: 73.559 Original_clip_loss: 0.23955 (0.22075) Loss: 0.23955 (0.22075) Dino_reguarizing_loss: 0.015344 (0.017499)\r\n",
      "2025-04-03,16:06:01 | INFO | Train Epoch: 3 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.087, 44.8363/s, 44.8363/s/gpu LR: 0.000004 Logit Scale: 73.572 Original_clip_loss: 0.29945 (0.24698) Loss: 0.29945 (0.24698) Dino_reguarizing_loss: 0.019263 (0.018087)\r\n",
      "2025-04-03,16:07:49 | INFO | Train Epoch: 3 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.085, 38.7775/s, 38.7775/s/gpu LR: 0.000004 Logit Scale: 73.581 Original_clip_loss: 0.21803 (0.23974) Loss: 0.21803 (0.23974) Dino_reguarizing_loss: 0.014552 (0.017203)\r\n",
      "2025-04-03,16:09:38 | INFO | Train Epoch: 3 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.083, 47.2700/s, 47.2700/s/gpu LR: 0.000004 Logit Scale: 73.587 Original_clip_loss: 0.30372 (0.25254) Loss: 0.30372 (0.25254) Dino_reguarizing_loss: 0.013921 (0.016547)\r\n",
      "2025-04-03,16:11:26 | INFO | Train Epoch: 3 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.088, 45.6727/s, 45.6727/s/gpu LR: 0.000004 Logit Scale: 73.594 Original_clip_loss: 0.17494 (0.23961) Loss: 0.17494 (0.23961) Dino_reguarizing_loss: 0.020740 (0.017246)\r\n",
      "2025-04-03,16:13:16 | INFO | Train Epoch: 3 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.098, 46.6926/s, 46.6926/s/gpu LR: 0.000004 Logit Scale: 73.602 Original_clip_loss: 0.22467 (0.23747) Loss: 0.22467 (0.23747) Dino_reguarizing_loss: 0.016087 (0.017080)\r\n",
      "2025-04-03,16:15:07 | INFO | Train Epoch: 3 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.105, 44.9030/s, 44.9030/s/gpu LR: 0.000004 Logit Scale: 73.611 Original_clip_loss: 0.34050 (0.25035) Loss: 0.34050 (0.25035) Dino_reguarizing_loss: 0.017033 (0.017074)\r\n",
      "2025-04-03,16:16:57 | INFO | Train Epoch: 3 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.099, 43.7729/s, 43.7729/s/gpu LR: 0.000004 Logit Scale: 73.616 Original_clip_loss: 0.38753 (0.26559) Loss: 0.38753 (0.26559) Dino_reguarizing_loss: 0.013742 (0.016704)\r\n",
      "2025-04-03,16:18:45 | INFO | Train Epoch: 3 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.088, 47.4242/s, 47.4242/s/gpu LR: 0.000004 Logit Scale: 73.620 Original_clip_loss: 0.15243 (0.25428) Loss: 0.15243 (0.25428) Dino_reguarizing_loss: 0.020600 (0.017093)\r\n",
      "2025-04-03,16:20:33 | INFO | Train Epoch: 3 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.077, 46.7532/s, 46.7532/s/gpu LR: 0.000004 Logit Scale: 73.627 Original_clip_loss: 0.32595 (0.26079) Loss: 0.32595 (0.26079) Dino_reguarizing_loss: 0.014823 (0.016887)\r\n",
      "2025-04-03,16:22:22 | INFO | Train Epoch: 3 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.092, 47.3235/s, 47.3235/s/gpu LR: 0.000004 Logit Scale: 73.629 Original_clip_loss: 0.15700 (0.25214) Loss: 0.15700 (0.25214) Dino_reguarizing_loss: 0.017806 (0.016964)\r\n",
      "2025-04-03,16:24:10 | INFO | Train Epoch: 3 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.081, 39.7732/s, 39.7732/s/gpu LR: 0.000004 Logit Scale: 73.633 Original_clip_loss: 0.25832 (0.25262) Loss: 0.25832 (0.25262) Dino_reguarizing_loss: 0.019825 (0.017184)\r\n",
      "2025-04-03,16:25:59 | INFO | Train Epoch: 3 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.084, 45.8088/s, 45.8088/s/gpu LR: 0.000004 Logit Scale: 73.636 Original_clip_loss: 0.25015 (0.25244) Loss: 0.25015 (0.25244) Dino_reguarizing_loss: 0.015317 (0.017050)\r\n",
      "2025-04-03,16:27:48 | INFO | Train Epoch: 3 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.087, 47.5217/s, 47.5217/s/gpu LR: 0.000004 Logit Scale: 73.643 Original_clip_loss: 0.39350 (0.26185) Loss: 0.39350 (0.26185) Dino_reguarizing_loss: 0.015749 (0.016964)\r\n",
      "2025-04-03,16:29:37 | INFO | Train Epoch: 3 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.091, 45.0600/s, 45.0600/s/gpu LR: 0.000004 Logit Scale: 73.643 Original_clip_loss: 0.18250 (0.25689) Loss: 0.18250 (0.25689) Dino_reguarizing_loss: 0.021782 (0.017265)\r\n",
      "2025-04-03,16:31:27 | INFO | Train Epoch: 3 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.099, 45.5064/s, 45.5064/s/gpu LR: 0.000004 Logit Scale: 73.650 Original_clip_loss: 0.24258 (0.25605) Loss: 0.24258 (0.25605) Dino_reguarizing_loss: 0.019428 (0.017392)\r\n",
      "2025-04-03,16:33:16 | INFO | Train Epoch: 3 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.090, 40.1708/s, 40.1708/s/gpu LR: 0.000004 Logit Scale: 73.654 Original_clip_loss: 0.22741 (0.25445) Loss: 0.22741 (0.25445) Dino_reguarizing_loss: 0.018719 (0.017466)\r\n",
      "2025-04-03,16:35:04 | INFO | Train Epoch: 3 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.083, 39.7426/s, 39.7426/s/gpu LR: 0.000004 Logit Scale: 73.659 Original_clip_loss: 0.21796 (0.25253) Loss: 0.21796 (0.25253) Dino_reguarizing_loss: 0.016723 (0.017427)\r\n",
      "2025-04-03,16:36:54 | INFO | Train Epoch: 3 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.097, 43.2166/s, 43.2166/s/gpu LR: 0.000004 Logit Scale: 73.662 Original_clip_loss: 0.36445 (0.25813) Loss: 0.36445 (0.25813) Dino_reguarizing_loss: 0.019002 (0.017505)\r\n",
      "2025-04-03,16:38:43 | INFO | Train Epoch: 3 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.093, 46.3411/s, 46.3411/s/gpu LR: 0.000004 Logit Scale: 73.664 Original_clip_loss: 0.28216 (0.25927) Loss: 0.28216 (0.25927) Dino_reguarizing_loss: 0.021955 (0.017717)\r\n",
      "2025-04-03,16:40:32 | INFO | Train Epoch: 3 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.093, 44.8640/s, 44.8640/s/gpu LR: 0.000004 Logit Scale: 73.665 Original_clip_loss: 0.30857 (0.26151) Loss: 0.30857 (0.26151) Dino_reguarizing_loss: 0.016542 (0.017664)\r\n",
      "2025-04-03,16:42:22 | INFO | Train Epoch: 3 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.095, 46.8777/s, 46.8777/s/gpu LR: 0.000004 Logit Scale: 73.670 Original_clip_loss: 0.37111 (0.26628) Loss: 0.37111 (0.26628) Dino_reguarizing_loss: 0.010555 (0.017355)\r\n",
      "2025-04-03,16:44:10 | INFO | Train Epoch: 3 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.088, 45.3481/s, 45.3481/s/gpu LR: 0.000004 Logit Scale: 73.676 Original_clip_loss: 0.20659 (0.26379) Loss: 0.20659 (0.26379) Dino_reguarizing_loss: 0.018819 (0.017416)\r\n",
      "2025-04-03,16:46:00 | INFO | Train Epoch: 3 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.093, 40.3157/s, 40.3157/s/gpu LR: 0.000004 Logit Scale: 73.677 Original_clip_loss: 0.39123 (0.26889) Loss: 0.39123 (0.26889) Dino_reguarizing_loss: 0.016991 (0.017399)\r\n",
      "2025-04-03,16:47:50 | INFO | Train Epoch: 3 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.098, 46.0534/s, 46.0534/s/gpu LR: 0.000004 Logit Scale: 73.681 Original_clip_loss: 0.18417 (0.26563) Loss: 0.18417 (0.26563) Dino_reguarizing_loss: 0.015799 (0.017337)\r\n",
      "2025-04-03,16:49:39 | INFO | Train Epoch: 3 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.090, 44.8570/s, 44.8570/s/gpu LR: 0.000004 Logit Scale: 73.688 Original_clip_loss: 0.27882 (0.26612) Loss: 0.27882 (0.26612) Dino_reguarizing_loss: 0.014869 (0.017246)\r\n",
      "2025-04-03,16:51:27 | INFO | Train Epoch: 3 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.089, 43.3082/s, 43.3082/s/gpu LR: 0.000004 Logit Scale: 73.689 Original_clip_loss: 0.20383 (0.26390) Loss: 0.20383 (0.26390) Dino_reguarizing_loss: 0.015593 (0.017187)\r\n",
      "2025-04-03,16:53:15 | INFO | Train Epoch: 3 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.080, 40.3681/s, 40.3681/s/gpu LR: 0.000003 Logit Scale: 73.696 Original_clip_loss: 0.19801 (0.26162) Loss: 0.19801 (0.26162) Dino_reguarizing_loss: 0.016277 (0.017155)\r\n",
      "2025-04-03,16:55:04 | INFO | Train Epoch: 3 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.087, 47.5122/s, 47.5122/s/gpu LR: 0.000003 Logit Scale: 73.697 Original_clip_loss: 0.17363 (0.25869) Loss: 0.17363 (0.25869) Dino_reguarizing_loss: 0.018477 (0.017200)\r\n",
      "2025-04-03,16:56:52 | INFO | Train Epoch: 3 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.081, 46.8641/s, 46.8641/s/gpu LR: 0.000003 Logit Scale: 73.698 Original_clip_loss: 0.23264 (0.25785) Loss: 0.23264 (0.25785) Dino_reguarizing_loss: 0.012940 (0.017062)\r\n",
      "2025-04-03,16:57:12 | INFO | Train Epoch: 3 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.065, 47.6316/s, 47.6316/s/gpu LR: 0.000003 Logit Scale: 73.697 Original_clip_loss: 0.11086 (0.25326) Loss: 0.11086 (0.25326) Dino_reguarizing_loss: 0.015389 (0.017010)\r\n",
      "2025-04-03,16:57:15 | INFO | Start epoch 4\r\n",
      "2025-04-03,16:57:17 | INFO | Train Epoch: 4 [    48/145000 (0%)] Data (t): 1.077 Batch (t): 2.343, 20.4886/s, 20.4886/s/gpu LR: 0.000003 Logit Scale: 73.697 Original_clip_loss: 0.15761 (0.15761) Loss: 0.15761 (0.15761) Dino_reguarizing_loss: 0.018350 (0.018350)\r\n",
      "2025-04-03,16:59:05 | INFO | Train Epoch: 4 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.082, 43.1587/s, 43.1587/s/gpu LR: 0.000003 Logit Scale: 73.710 Original_clip_loss: 0.10876 (0.13319) Loss: 0.10876 (0.13319) Dino_reguarizing_loss: 0.016993 (0.017671)\r\n",
      "2025-04-03,17:00:55 | INFO | Train Epoch: 4 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.098, 46.9895/s, 46.9895/s/gpu LR: 0.000003 Logit Scale: 73.717 Original_clip_loss: 0.17791 (0.14809) Loss: 0.17791 (0.14809) Dino_reguarizing_loss: 0.014541 (0.016628)\r\n",
      "2025-04-03,17:02:44 | INFO | Train Epoch: 4 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.092, 42.5607/s, 42.5607/s/gpu LR: 0.000003 Logit Scale: 73.726 Original_clip_loss: 0.25574 (0.17501) Loss: 0.25574 (0.17501) Dino_reguarizing_loss: 0.015976 (0.016465)\r\n",
      "2025-04-03,17:04:32 | INFO | Train Epoch: 4 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.074, 44.9987/s, 44.9987/s/gpu LR: 0.000003 Logit Scale: 73.733 Original_clip_loss: 0.13873 (0.16775) Loss: 0.13873 (0.16775) Dino_reguarizing_loss: 0.018173 (0.016807)\r\n",
      "2025-04-03,17:06:20 | INFO | Train Epoch: 4 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.085, 41.1707/s, 41.1707/s/gpu LR: 0.000003 Logit Scale: 73.736 Original_clip_loss: 0.16322 (0.16699) Loss: 0.16322 (0.16699) Dino_reguarizing_loss: 0.014351 (0.016397)\r\n",
      "2025-04-03,17:08:10 | INFO | Train Epoch: 4 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.098, 44.9925/s, 44.9925/s/gpu LR: 0.000003 Logit Scale: 73.744 Original_clip_loss: 0.050830 (0.15040) Loss: 0.050830 (0.15040) Dino_reguarizing_loss: 0.014402 (0.016112)\r\n",
      "2025-04-03,17:10:00 | INFO | Train Epoch: 4 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.100, 46.7865/s, 46.7865/s/gpu LR: 0.000003 Logit Scale: 73.752 Original_clip_loss: 0.032525 (0.13567) Loss: 0.032525 (0.13567) Dino_reguarizing_loss: 0.017353 (0.016267)\r\n",
      "2025-04-03,17:11:48 | INFO | Train Epoch: 4 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.084, 44.9693/s, 44.9693/s/gpu LR: 0.000003 Logit Scale: 73.756 Original_clip_loss: 0.15174 (0.13745) Loss: 0.15174 (0.13745) Dino_reguarizing_loss: 0.018694 (0.016537)\r\n",
      "2025-04-03,17:13:38 | INFO | Train Epoch: 4 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.096, 39.8024/s, 39.8024/s/gpu LR: 0.000003 Logit Scale: 73.760 Original_clip_loss: 0.15625 (0.13933) Loss: 0.15625 (0.13933) Dino_reguarizing_loss: 0.016537 (0.016537)\r\n",
      "2025-04-03,17:15:28 | INFO | Train Epoch: 4 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.095, 45.5532/s, 45.5532/s/gpu LR: 0.000003 Logit Scale: 73.769 Original_clip_loss: 0.13952 (0.13935) Loss: 0.13952 (0.13935) Dino_reguarizing_loss: 0.014374 (0.016340)\r\n",
      "2025-04-03,17:17:17 | INFO | Train Epoch: 4 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.091, 41.0090/s, 41.0090/s/gpu LR: 0.000003 Logit Scale: 73.777 Original_clip_loss: 0.31824 (0.15426) Loss: 0.31824 (0.15426) Dino_reguarizing_loss: 0.015362 (0.016259)\r\n",
      "2025-04-03,17:19:07 | INFO | Train Epoch: 4 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.100, 42.6239/s, 42.6239/s/gpu LR: 0.000003 Logit Scale: 73.782 Original_clip_loss: 0.18435 (0.15657) Loss: 0.18435 (0.15657) Dino_reguarizing_loss: 0.018017 (0.016394)\r\n",
      "2025-04-03,17:20:56 | INFO | Train Epoch: 4 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.089, 39.7480/s, 39.7480/s/gpu LR: 0.000003 Logit Scale: 73.787 Original_clip_loss: 0.14951 (0.15607) Loss: 0.14951 (0.15607) Dino_reguarizing_loss: 0.012047 (0.016084)\r\n",
      "2025-04-03,17:22:44 | INFO | Train Epoch: 4 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.082, 47.4853/s, 47.4853/s/gpu LR: 0.000003 Logit Scale: 73.789 Original_clip_loss: 0.31268 (0.16651) Loss: 0.31268 (0.16651) Dino_reguarizing_loss: 0.014945 (0.016008)\r\n",
      "2025-04-03,17:24:32 | INFO | Train Epoch: 4 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.080, 40.9626/s, 40.9626/s/gpu LR: 0.000003 Logit Scale: 73.793 Original_clip_loss: 0.14361 (0.16508) Loss: 0.14361 (0.16508) Dino_reguarizing_loss: 0.015362 (0.015967)\r\n",
      "2025-04-03,17:26:20 | INFO | Train Epoch: 4 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.080, 46.4236/s, 46.4236/s/gpu LR: 0.000003 Logit Scale: 73.801 Original_clip_loss: 0.080857 (0.16012) Loss: 0.080857 (0.16012) Dino_reguarizing_loss: 0.014615 (0.015888)\r\n",
      "2025-04-03,17:28:09 | INFO | Train Epoch: 4 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.093, 46.8135/s, 46.8135/s/gpu LR: 0.000003 Logit Scale: 73.807 Original_clip_loss: 0.15123 (0.15963) Loss: 0.15123 (0.15963) Dino_reguarizing_loss: 0.017648 (0.015986)\r\n",
      "2025-04-03,17:29:57 | INFO | Train Epoch: 4 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.082, 47.6421/s, 47.6421/s/gpu LR: 0.000003 Logit Scale: 73.810 Original_clip_loss: 0.074131 (0.15513) Loss: 0.074131 (0.15513) Dino_reguarizing_loss: 0.018647 (0.016126)\r\n",
      "2025-04-03,17:31:47 | INFO | Train Epoch: 4 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.103, 41.0807/s, 41.0807/s/gpu LR: 0.000003 Logit Scale: 73.812 Original_clip_loss: 0.19601 (0.15717) Loss: 0.19601 (0.15717) Dino_reguarizing_loss: 0.016521 (0.016145)\r\n",
      "2025-04-03,17:33:38 | INFO | Train Epoch: 4 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.103, 46.7496/s, 46.7496/s/gpu LR: 0.000003 Logit Scale: 73.817 Original_clip_loss: 0.21997 (0.16016) Loss: 0.21997 (0.16016) Dino_reguarizing_loss: 0.018751 (0.016270)\r\n",
      "2025-04-03,17:35:27 | INFO | Train Epoch: 4 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.089, 41.0115/s, 41.0115/s/gpu LR: 0.000003 Logit Scale: 73.821 Original_clip_loss: 0.22377 (0.16305) Loss: 0.22377 (0.16305) Dino_reguarizing_loss: 0.016181 (0.016265)\r\n",
      "2025-04-03,17:37:17 | INFO | Train Epoch: 4 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.108, 39.7301/s, 39.7301/s/gpu LR: 0.000003 Logit Scale: 73.824 Original_clip_loss: 0.31487 (0.16965) Loss: 0.31487 (0.16965) Dino_reguarizing_loss: 0.017890 (0.016336)\r\n",
      "2025-04-03,17:39:07 | INFO | Train Epoch: 4 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.098, 40.3846/s, 40.3846/s/gpu LR: 0.000003 Logit Scale: 73.829 Original_clip_loss: 0.087698 (0.16624) Loss: 0.087698 (0.16624) Dino_reguarizing_loss: 0.016500 (0.016343)\r\n",
      "2025-04-03,17:40:57 | INFO | Train Epoch: 4 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.095, 41.0763/s, 41.0763/s/gpu LR: 0.000003 Logit Scale: 73.835 Original_clip_loss: 0.21334 (0.16812) Loss: 0.21334 (0.16812) Dino_reguarizing_loss: 0.015489 (0.016309)\r\n",
      "2025-04-03,17:42:47 | INFO | Train Epoch: 4 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.101, 43.4447/s, 43.4447/s/gpu LR: 0.000003 Logit Scale: 73.840 Original_clip_loss: 0.10102 (0.16554) Loss: 0.10102 (0.16554) Dino_reguarizing_loss: 0.016466 (0.016315)\r\n",
      "2025-04-03,17:44:36 | INFO | Train Epoch: 4 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.093, 42.5341/s, 42.5341/s/gpu LR: 0.000003 Logit Scale: 73.843 Original_clip_loss: 0.30097 (0.17056) Loss: 0.30097 (0.17056) Dino_reguarizing_loss: 0.019049 (0.016416)\r\n",
      "2025-04-03,17:46:26 | INFO | Train Epoch: 4 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.095, 47.0144/s, 47.0144/s/gpu LR: 0.000003 Logit Scale: 73.846 Original_clip_loss: 0.21456 (0.17213) Loss: 0.21456 (0.17213) Dino_reguarizing_loss: 0.019495 (0.016526)\r\n",
      "2025-04-03,17:48:15 | INFO | Train Epoch: 4 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.088, 43.2888/s, 43.2888/s/gpu LR: 0.000003 Logit Scale: 73.851 Original_clip_loss: 0.11120 (0.17003) Loss: 0.11120 (0.17003) Dino_reguarizing_loss: 0.018103 (0.016580)\r\n",
      "2025-04-03,17:50:04 | INFO | Train Epoch: 4 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.096, 41.0241/s, 41.0241/s/gpu LR: 0.000003 Logit Scale: 73.851 Original_clip_loss: 0.30429 (0.17450) Loss: 0.30429 (0.17450) Dino_reguarizing_loss: 0.016758 (0.016586)\r\n",
      "2025-04-03,17:51:52 | INFO | Train Epoch: 4 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.081, 47.5459/s, 47.5459/s/gpu LR: 0.000003 Logit Scale: 73.856 Original_clip_loss: 0.11836 (0.17269) Loss: 0.11836 (0.17269) Dino_reguarizing_loss: 0.017492 (0.016616)\r\n",
      "2025-04-03,17:52:12 | INFO | Train Epoch: 4 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.038, 47.9362/s, 47.9362/s/gpu LR: 0.000003 Logit Scale: 73.855 Original_clip_loss: 0.16159 (0.17235) Loss: 0.16159 (0.17235) Dino_reguarizing_loss: 0.014429 (0.016547)\r\n",
      "2025-04-03,17:52:14 | INFO | Start epoch 5\r\n",
      "2025-04-03,17:52:16 | INFO | Train Epoch: 5 [    48/145000 (0%)] Data (t): 0.928 Batch (t): 2.157, 22.2576/s, 22.2576/s/gpu LR: 0.000003 Logit Scale: 73.855 Original_clip_loss: 0.11447 (0.11447) Loss: 0.11447 (0.11447) Dino_reguarizing_loss: 0.017142 (0.017142)\r\n",
      "2025-04-03,17:54:05 | INFO | Train Epoch: 5 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.091, 46.8452/s, 46.8452/s/gpu LR: 0.000003 Logit Scale: 73.860 Original_clip_loss: 0.088120 (0.10130) Loss: 0.088120 (0.10130) Dino_reguarizing_loss: 0.016445 (0.016794)\r\n",
      "2025-04-03,17:55:54 | INFO | Train Epoch: 5 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.090, 40.1691/s, 40.1691/s/gpu LR: 0.000003 Logit Scale: 73.870 Original_clip_loss: 0.16830 (0.12363) Loss: 0.16830 (0.12363) Dino_reguarizing_loss: 0.021649 (0.018412)\r\n",
      "2025-04-03,17:57:44 | INFO | Train Epoch: 5 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.094, 46.5715/s, 46.5715/s/gpu LR: 0.000003 Logit Scale: 73.877 Original_clip_loss: 0.23815 (0.15226) Loss: 0.23815 (0.15226) Dino_reguarizing_loss: 0.019392 (0.018657)\r\n",
      "2025-04-03,17:59:33 | INFO | Train Epoch: 5 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.097, 40.3602/s, 40.3602/s/gpu LR: 0.000003 Logit Scale: 73.885 Original_clip_loss: 0.28119 (0.17804) Loss: 0.28119 (0.17804) Dino_reguarizing_loss: 0.013940 (0.017714)\r\n",
      "2025-04-03,18:01:22 | INFO | Train Epoch: 5 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.090, 46.6818/s, 46.6818/s/gpu LR: 0.000003 Logit Scale: 73.892 Original_clip_loss: 0.12942 (0.16994) Loss: 0.12942 (0.16994) Dino_reguarizing_loss: 0.014844 (0.017235)\r\n",
      "2025-04-03,18:03:11 | INFO | Train Epoch: 5 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.084, 44.0825/s, 44.0825/s/gpu LR: 0.000002 Logit Scale: 73.897 Original_clip_loss: 0.038734 (0.15120) Loss: 0.038734 (0.15120) Dino_reguarizing_loss: 0.014153 (0.016795)\r\n",
      "2025-04-03,18:04:59 | INFO | Train Epoch: 5 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.083, 45.0816/s, 45.0816/s/gpu LR: 0.000002 Logit Scale: 73.903 Original_clip_loss: 0.20770 (0.15826) Loss: 0.20770 (0.15826) Dino_reguarizing_loss: 0.011916 (0.016185)\r\n",
      "2025-04-03,18:06:47 | INFO | Train Epoch: 5 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.078, 40.5409/s, 40.5409/s/gpu LR: 0.000002 Logit Scale: 73.907 Original_clip_loss: 0.14371 (0.15664) Loss: 0.14371 (0.15664) Dino_reguarizing_loss: 0.016268 (0.016194)\r\n",
      "2025-04-03,18:08:34 | INFO | Train Epoch: 5 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.073, 47.5437/s, 47.5437/s/gpu LR: 0.000002 Logit Scale: 73.911 Original_clip_loss: 0.15200 (0.15618) Loss: 0.15200 (0.15618) Dino_reguarizing_loss: 0.016212 (0.016196)\r\n",
      "2025-04-03,18:10:21 | INFO | Train Epoch: 5 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.072, 46.5985/s, 46.5985/s/gpu LR: 0.000002 Logit Scale: 73.914 Original_clip_loss: 0.14213 (0.15490) Loss: 0.14213 (0.15490) Dino_reguarizing_loss: 0.013237 (0.015927)\r\n",
      "2025-04-03,18:12:11 | INFO | Train Epoch: 5 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.095, 46.7205/s, 46.7205/s/gpu LR: 0.000002 Logit Scale: 73.919 Original_clip_loss: 0.12563 (0.15246) Loss: 0.12563 (0.15246) Dino_reguarizing_loss: 0.015928 (0.015927)\r\n",
      "2025-04-03,18:13:58 | INFO | Train Epoch: 5 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.076, 45.9483/s, 45.9483/s/gpu LR: 0.000002 Logit Scale: 73.925 Original_clip_loss: 0.082476 (0.14708) Loss: 0.082476 (0.14708) Dino_reguarizing_loss: 0.016692 (0.015986)\r\n",
      "2025-04-03,18:15:47 | INFO | Train Epoch: 5 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.091, 40.2259/s, 40.2259/s/gpu LR: 0.000002 Logit Scale: 73.931 Original_clip_loss: 0.10913 (0.14437) Loss: 0.10913 (0.14437) Dino_reguarizing_loss: 0.012180 (0.015714)\r\n",
      "2025-04-03,18:17:36 | INFO | Train Epoch: 5 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.084, 47.2664/s, 47.2664/s/gpu LR: 0.000002 Logit Scale: 73.935 Original_clip_loss: 0.23375 (0.15033) Loss: 0.23375 (0.15033) Dino_reguarizing_loss: 0.015913 (0.015727)\r\n",
      "2025-04-03,18:19:25 | INFO | Train Epoch: 5 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.093, 47.4194/s, 47.4194/s/gpu LR: 0.000002 Logit Scale: 73.941 Original_clip_loss: 0.14630 (0.15008) Loss: 0.14630 (0.15008) Dino_reguarizing_loss: 0.011799 (0.015482)\r\n",
      "2025-04-03,18:21:13 | INFO | Train Epoch: 5 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.082, 46.5570/s, 46.5570/s/gpu LR: 0.000002 Logit Scale: 73.947 Original_clip_loss: 0.13064 (0.14893) Loss: 0.13064 (0.14893) Dino_reguarizing_loss: 0.011424 (0.015243)\r\n",
      "2025-04-03,18:23:02 | INFO | Train Epoch: 5 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.089, 39.0344/s, 39.0344/s/gpu LR: 0.000002 Logit Scale: 73.950 Original_clip_loss: 0.20360 (0.15197) Loss: 0.20360 (0.15197) Dino_reguarizing_loss: 0.016638 (0.015321)\r\n",
      "2025-04-03,18:24:50 | INFO | Train Epoch: 5 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.080, 46.6535/s, 46.6535/s/gpu LR: 0.000002 Logit Scale: 73.953 Original_clip_loss: 0.22226 (0.15567) Loss: 0.22226 (0.15567) Dino_reguarizing_loss: 0.013033 (0.015200)\r\n",
      "2025-04-03,18:26:40 | INFO | Train Epoch: 5 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.095, 46.4141/s, 46.4141/s/gpu LR: 0.000002 Logit Scale: 73.954 Original_clip_loss: 0.18578 (0.15717) Loss: 0.18578 (0.15717) Dino_reguarizing_loss: 0.014822 (0.015181)\r\n",
      "2025-04-03,18:28:30 | INFO | Train Epoch: 5 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.102, 42.6185/s, 42.6185/s/gpu LR: 0.000002 Logit Scale: 73.959 Original_clip_loss: 0.13897 (0.15631) Loss: 0.13897 (0.15631) Dino_reguarizing_loss: 0.016616 (0.015250)\r\n",
      "2025-04-03,18:30:18 | INFO | Train Epoch: 5 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.082, 46.5822/s, 46.5822/s/gpu LR: 0.000002 Logit Scale: 73.964 Original_clip_loss: 0.13596 (0.15538) Loss: 0.13596 (0.15538) Dino_reguarizing_loss: 0.015823 (0.015276)\r\n",
      "2025-04-03,18:32:08 | INFO | Train Epoch: 5 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.095, 44.0179/s, 44.0179/s/gpu LR: 0.000002 Logit Scale: 73.969 Original_clip_loss: 0.052217 (0.15090) Loss: 0.052217 (0.15090) Dino_reguarizing_loss: 0.013945 (0.015218)\r\n",
      "2025-04-03,18:33:56 | INFO | Train Epoch: 5 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.088, 45.0583/s, 45.0583/s/gpu LR: 0.000002 Logit Scale: 73.973 Original_clip_loss: 0.10546 (0.14900) Loss: 0.10546 (0.14900) Dino_reguarizing_loss: 0.012449 (0.015103)\r\n",
      "2025-04-03,18:35:47 | INFO | Train Epoch: 5 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.106, 40.6277/s, 40.6277/s/gpu LR: 0.000002 Logit Scale: 73.977 Original_clip_loss: 0.035551 (0.14447) Loss: 0.035551 (0.14447) Dino_reguarizing_loss: 0.015012 (0.015099)\r\n",
      "2025-04-03,18:37:36 | INFO | Train Epoch: 5 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.092, 40.1598/s, 40.1598/s/gpu LR: 0.000002 Logit Scale: 73.979 Original_clip_loss: 0.39410 (0.15407) Loss: 0.39410 (0.15407) Dino_reguarizing_loss: 0.015463 (0.015113)\r\n",
      "2025-04-03,18:39:25 | INFO | Train Epoch: 5 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.089, 46.8418/s, 46.8418/s/gpu LR: 0.000002 Logit Scale: 73.981 Original_clip_loss: 0.16390 (0.15443) Loss: 0.16390 (0.15443) Dino_reguarizing_loss: 0.013762 (0.015063)\r\n",
      "2025-04-03,18:41:13 | INFO | Train Epoch: 5 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.083, 44.0863/s, 44.0863/s/gpu LR: 0.000002 Logit Scale: 73.986 Original_clip_loss: 0.23643 (0.15736) Loss: 0.23643 (0.15736) Dino_reguarizing_loss: 0.013833 (0.015019)\r\n",
      "2025-04-03,18:43:01 | INFO | Train Epoch: 5 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.081, 46.4576/s, 46.4576/s/gpu LR: 0.000002 Logit Scale: 73.989 Original_clip_loss: 0.19757 (0.15875) Loss: 0.19757 (0.15875) Dino_reguarizing_loss: 0.015849 (0.015048)\r\n",
      "2025-04-03,18:44:52 | INFO | Train Epoch: 5 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.105, 42.4688/s, 42.4688/s/gpu LR: 0.000002 Logit Scale: 73.992 Original_clip_loss: 0.25018 (0.16179) Loss: 0.25018 (0.16179) Dino_reguarizing_loss: 0.018363 (0.015158)\r\n",
      "2025-04-03,18:46:42 | INFO | Train Epoch: 5 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.096, 41.9656/s, 41.9656/s/gpu LR: 0.000002 Logit Scale: 73.995 Original_clip_loss: 0.22872 (0.16395) Loss: 0.22872 (0.16395) Dino_reguarizing_loss: 0.016349 (0.015196)\r\n",
      "2025-04-03,18:47:01 | INFO | Train Epoch: 5 [144960/145000 (100%)] Data (t): 0.002 Batch (t): 1.041, 47.7771/s, 47.7771/s/gpu LR: 0.000002 Logit Scale: 73.995 Original_clip_loss: 0.090979 (0.16167) Loss: 0.090979 (0.16167) Dino_reguarizing_loss: 0.016763 (0.015245)\r\n",
      "2025-04-03,18:47:03 | INFO | Start epoch 6\r\n",
      "2025-04-03,18:47:06 | INFO | Train Epoch: 6 [    48/145000 (0%)] Data (t): 0.947 Batch (t): 2.226, 21.5653/s, 21.5653/s/gpu LR: 0.000002 Logit Scale: 73.995 Original_clip_loss: 0.098855 (0.098855) Loss: 0.098855 (0.098855) Dino_reguarizing_loss: 0.014357 (0.014357)\r\n",
      "2025-04-03,18:48:56 | INFO | Train Epoch: 6 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.103, 40.4787/s, 40.4787/s/gpu LR: 0.000002 Logit Scale: 73.999 Original_clip_loss: 0.16299 (0.13092) Loss: 0.16299 (0.13092) Dino_reguarizing_loss: 0.016426 (0.015392)\r\n",
      "2025-04-03,18:50:44 | INFO | Train Epoch: 6 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.078, 46.7353/s, 46.7353/s/gpu LR: 0.000002 Logit Scale: 74.001 Original_clip_loss: 0.049734 (0.10386) Loss: 0.049734 (0.10386) Dino_reguarizing_loss: 0.015959 (0.015581)\r\n",
      "2025-04-03,18:52:32 | INFO | Train Epoch: 6 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.079, 44.9656/s, 44.9656/s/gpu LR: 0.000002 Logit Scale: 74.005 Original_clip_loss: 0.11999 (0.10789) Loss: 0.11999 (0.10789) Dino_reguarizing_loss: 0.018515 (0.016314)\r\n",
      "2025-04-03,18:54:21 | INFO | Train Epoch: 6 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.093, 46.8612/s, 46.8612/s/gpu LR: 0.000002 Logit Scale: 74.009 Original_clip_loss: 0.11232 (0.10878) Loss: 0.11232 (0.10878) Dino_reguarizing_loss: 0.014836 (0.016019)\r\n",
      "2025-04-03,18:56:09 | INFO | Train Epoch: 6 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.087, 45.6304/s, 45.6304/s/gpu LR: 0.000002 Logit Scale: 74.012 Original_clip_loss: 0.19799 (0.12365) Loss: 0.19799 (0.12365) Dino_reguarizing_loss: 0.015114 (0.015868)\r\n",
      "2025-04-03,18:57:57 | INFO | Train Epoch: 6 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.071, 43.4058/s, 43.4058/s/gpu LR: 0.000002 Logit Scale: 74.016 Original_clip_loss: 0.16456 (0.12949) Loss: 0.16456 (0.12949) Dino_reguarizing_loss: 0.011700 (0.015272)\r\n",
      "2025-04-03,18:59:46 | INFO | Train Epoch: 6 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.094, 41.8057/s, 41.8057/s/gpu LR: 0.000002 Logit Scale: 74.019 Original_clip_loss: 0.29533 (0.15022) Loss: 0.29533 (0.15022) Dino_reguarizing_loss: 0.015577 (0.015311)\r\n",
      "2025-04-03,19:01:35 | INFO | Train Epoch: 6 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.093, 48.4041/s, 48.4041/s/gpu LR: 0.000002 Logit Scale: 74.023 Original_clip_loss: 0.11697 (0.14653) Loss: 0.11697 (0.14653) Dino_reguarizing_loss: 0.017275 (0.015529)\r\n",
      "2025-04-03,19:03:24 | INFO | Train Epoch: 6 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.090, 47.0450/s, 47.0450/s/gpu LR: 0.000002 Logit Scale: 74.025 Original_clip_loss: 0.070852 (0.13896) Loss: 0.070852 (0.13896) Dino_reguarizing_loss: 0.016489 (0.015625)\r\n",
      "2025-04-03,19:05:13 | INFO | Train Epoch: 6 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.090, 47.7911/s, 47.7911/s/gpu LR: 0.000002 Logit Scale: 74.028 Original_clip_loss: 0.13169 (0.13830) Loss: 0.13169 (0.13830) Dino_reguarizing_loss: 0.014057 (0.015482)\r\n",
      "2025-04-03,19:07:02 | INFO | Train Epoch: 6 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.084, 38.9966/s, 38.9966/s/gpu LR: 0.000002 Logit Scale: 74.030 Original_clip_loss: 0.15269 (0.13950) Loss: 0.15269 (0.13950) Dino_reguarizing_loss: 0.013069 (0.015281)\r\n",
      "2025-04-03,19:08:50 | INFO | Train Epoch: 6 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.088, 40.2511/s, 40.2511/s/gpu LR: 0.000002 Logit Scale: 74.033 Original_clip_loss: 0.17539 (0.14226) Loss: 0.17539 (0.14226) Dino_reguarizing_loss: 0.012115 (0.015038)\r\n",
      "2025-04-03,19:10:40 | INFO | Train Epoch: 6 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.099, 45.7629/s, 45.7629/s/gpu LR: 0.000002 Logit Scale: 74.036 Original_clip_loss: 0.14670 (0.14258) Loss: 0.14670 (0.14258) Dino_reguarizing_loss: 0.018518 (0.015286)\r\n",
      "2025-04-03,19:12:29 | INFO | Train Epoch: 6 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.086, 46.6641/s, 46.6641/s/gpu LR: 0.000001 Logit Scale: 74.040 Original_clip_loss: 0.20753 (0.14691) Loss: 0.20753 (0.14691) Dino_reguarizing_loss: 0.018093 (0.015473)\r\n",
      "2025-04-03,19:14:17 | INFO | Train Epoch: 6 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.082, 42.4130/s, 42.4130/s/gpu LR: 0.000001 Logit Scale: 74.043 Original_clip_loss: 0.10817 (0.14448) Loss: 0.10817 (0.14448) Dino_reguarizing_loss: 0.014619 (0.015420)\r\n",
      "2025-04-03,19:16:08 | INFO | Train Epoch: 6 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.108, 44.2183/s, 44.2183/s/gpu LR: 0.000001 Logit Scale: 74.043 Original_clip_loss: 0.091723 (0.14138) Loss: 0.091723 (0.14138) Dino_reguarizing_loss: 0.015350 (0.015416)\r\n",
      "2025-04-03,19:17:58 | INFO | Train Epoch: 6 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.099, 42.4744/s, 42.4744/s/gpu LR: 0.000001 Logit Scale: 74.046 Original_clip_loss: 0.25549 (0.14772) Loss: 0.25549 (0.14772) Dino_reguarizing_loss: 0.018097 (0.015565)\r\n",
      "2025-04-03,19:19:45 | INFO | Train Epoch: 6 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.073, 47.2405/s, 47.2405/s/gpu LR: 0.000001 Logit Scale: 74.050 Original_clip_loss: 0.11604 (0.14605) Loss: 0.11604 (0.14605) Dino_reguarizing_loss: 0.016349 (0.015606)\r\n",
      "2025-04-03,19:21:33 | INFO | Train Epoch: 6 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.084, 46.8173/s, 46.8173/s/gpu LR: 0.000001 Logit Scale: 74.054 Original_clip_loss: 0.13477 (0.14549) Loss: 0.13477 (0.14549) Dino_reguarizing_loss: 0.016180 (0.015635)\r\n",
      "2025-04-03,19:23:21 | INFO | Train Epoch: 6 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.075, 44.9536/s, 44.9536/s/gpu LR: 0.000001 Logit Scale: 74.058 Original_clip_loss: 0.15938 (0.14615) Loss: 0.15938 (0.14615) Dino_reguarizing_loss: 0.015808 (0.015643)\r\n",
      "2025-04-03,19:25:10 | INFO | Train Epoch: 6 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.091, 46.9020/s, 46.9020/s/gpu LR: 0.000001 Logit Scale: 74.062 Original_clip_loss: 0.081994 (0.14323) Loss: 0.081994 (0.14323) Dino_reguarizing_loss: 0.011191 (0.015441)\r\n",
      "2025-04-03,19:26:58 | INFO | Train Epoch: 6 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.080, 44.1881/s, 44.1881/s/gpu LR: 0.000001 Logit Scale: 74.064 Original_clip_loss: 0.065268 (0.13984) Loss: 0.065268 (0.13984) Dino_reguarizing_loss: 0.013788 (0.015369)\r\n",
      "2025-04-03,19:28:46 | INFO | Train Epoch: 6 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.082, 46.8792/s, 46.8792/s/gpu LR: 0.000001 Logit Scale: 74.067 Original_clip_loss: 0.18585 (0.14176) Loss: 0.18585 (0.14176) Dino_reguarizing_loss: 0.013149 (0.015276)\r\n",
      "2025-04-03,19:30:35 | INFO | Train Epoch: 6 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.093, 41.1472/s, 41.1472/s/gpu LR: 0.000001 Logit Scale: 74.070 Original_clip_loss: 0.23763 (0.14559) Loss: 0.23763 (0.14559) Dino_reguarizing_loss: 0.011752 (0.015135)\r\n",
      "2025-04-03,19:32:23 | INFO | Train Epoch: 6 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.079, 45.8840/s, 45.8840/s/gpu LR: 0.000001 Logit Scale: 74.071 Original_clip_loss: 0.079237 (0.14304) Loss: 0.079237 (0.14304) Dino_reguarizing_loss: 0.012963 (0.015052)\r\n",
      "2025-04-03,19:34:12 | INFO | Train Epoch: 6 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.090, 46.7236/s, 46.7236/s/gpu LR: 0.000001 Logit Scale: 74.073 Original_clip_loss: 0.10279 (0.14155) Loss: 0.10279 (0.14155) Dino_reguarizing_loss: 0.014504 (0.015032)\r\n",
      "2025-04-03,19:36:01 | INFO | Train Epoch: 6 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.090, 40.0676/s, 40.0676/s/gpu LR: 0.000001 Logit Scale: 74.075 Original_clip_loss: 0.20139 (0.14369) Loss: 0.20139 (0.14369) Dino_reguarizing_loss: 0.013202 (0.014966)\r\n",
      "2025-04-03,19:37:51 | INFO | Train Epoch: 6 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.098, 46.8185/s, 46.8185/s/gpu LR: 0.000001 Logit Scale: 74.078 Original_clip_loss: 0.066107 (0.14101) Loss: 0.066107 (0.14101) Dino_reguarizing_loss: 0.014459 (0.014949)\r\n",
      "2025-04-03,19:39:39 | INFO | Train Epoch: 6 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.078, 44.8942/s, 44.8942/s/gpu LR: 0.000001 Logit Scale: 74.078 Original_clip_loss: 0.15999 (0.14165) Loss: 0.15999 (0.14165) Dino_reguarizing_loss: 0.013023 (0.014885)\r\n",
      "2025-04-03,19:41:27 | INFO | Train Epoch: 6 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.086, 44.2633/s, 44.2633/s/gpu LR: 0.000001 Logit Scale: 74.079 Original_clip_loss: 0.11813 (0.14089) Loss: 0.11813 (0.14089) Dino_reguarizing_loss: 0.014073 (0.014858)\r\n",
      "2025-04-03,19:41:47 | INFO | Train Epoch: 6 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.030, 47.8402/s, 47.8402/s/gpu LR: 0.000001 Logit Scale: 74.079 Original_clip_loss: 0.15359 (0.14128) Loss: 0.15359 (0.14128) Dino_reguarizing_loss: 0.015262 (0.014871)\r\n",
      "2025-04-03,19:41:49 | INFO | Start epoch 7\r\n",
      "2025-04-03,19:41:51 | INFO | Train Epoch: 7 [    48/145000 (0%)] Data (t): 1.070 Batch (t): 2.451, 19.5839/s, 19.5839/s/gpu LR: 0.000001 Logit Scale: 74.079 Original_clip_loss: 0.093192 (0.093192) Loss: 0.093192 (0.093192) Dino_reguarizing_loss: 0.015888 (0.015888)\r\n",
      "2025-04-03,19:43:40 | INFO | Train Epoch: 7 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.086, 46.2760/s, 46.2760/s/gpu LR: 0.000001 Logit Scale: 74.083 Original_clip_loss: 0.18080 (0.13700) Loss: 0.18080 (0.13700) Dino_reguarizing_loss: 0.017296 (0.016592)\r\n",
      "2025-04-03,19:45:29 | INFO | Train Epoch: 7 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.086, 47.4679/s, 47.4679/s/gpu LR: 0.000001 Logit Scale: 74.085 Original_clip_loss: 0.089642 (0.12121) Loss: 0.089642 (0.12121) Dino_reguarizing_loss: 0.014371 (0.015852)\r\n",
      "2025-04-03,19:47:18 | INFO | Train Epoch: 7 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.089, 46.6528/s, 46.6528/s/gpu LR: 0.000001 Logit Scale: 74.086 Original_clip_loss: 0.090726 (0.11359) Loss: 0.090726 (0.11359) Dino_reguarizing_loss: 0.014220 (0.015444)\r\n",
      "2025-04-03,19:49:07 | INFO | Train Epoch: 7 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.095, 47.7291/s, 47.7291/s/gpu LR: 0.000001 Logit Scale: 74.089 Original_clip_loss: 0.13491 (0.11785) Loss: 0.13491 (0.11785) Dino_reguarizing_loss: 0.013338 (0.015023)\r\n",
      "2025-04-03,19:50:57 | INFO | Train Epoch: 7 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.097, 46.6071/s, 46.6071/s/gpu LR: 0.000001 Logit Scale: 74.092 Original_clip_loss: 0.12256 (0.11864) Loss: 0.12256 (0.11864) Dino_reguarizing_loss: 0.017295 (0.015401)\r\n",
      "2025-04-03,19:52:47 | INFO | Train Epoch: 7 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.099, 42.4037/s, 42.4037/s/gpu LR: 0.000001 Logit Scale: 74.095 Original_clip_loss: 0.056678 (0.10979) Loss: 0.056678 (0.10979) Dino_reguarizing_loss: 0.015596 (0.015429)\r\n",
      "2025-04-03,19:54:35 | INFO | Train Epoch: 7 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.087, 46.7911/s, 46.7911/s/gpu LR: 0.000001 Logit Scale: 74.096 Original_clip_loss: 0.15272 (0.11515) Loss: 0.15272 (0.11515) Dino_reguarizing_loss: 0.015962 (0.015496)\r\n",
      "2025-04-03,19:56:25 | INFO | Train Epoch: 7 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.091, 40.1490/s, 40.1490/s/gpu LR: 0.000001 Logit Scale: 74.098 Original_clip_loss: 0.067800 (0.10989) Loss: 0.067800 (0.10989) Dino_reguarizing_loss: 0.013274 (0.015249)\r\n",
      "2025-04-03,19:58:13 | INFO | Train Epoch: 7 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.089, 46.6433/s, 46.6433/s/gpu LR: 0.000001 Logit Scale: 74.101 Original_clip_loss: 0.034022 (0.10231) Loss: 0.034022 (0.10231) Dino_reguarizing_loss: 0.013235 (0.015048)\r\n",
      "2025-04-03,20:00:01 | INFO | Train Epoch: 7 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.075, 46.5175/s, 46.5175/s/gpu LR: 0.000001 Logit Scale: 74.104 Original_clip_loss: 0.11862 (0.10379) Loss: 0.11862 (0.10379) Dino_reguarizing_loss: 0.013039 (0.014865)\r\n",
      "2025-04-03,20:01:49 | INFO | Train Epoch: 7 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.084, 44.7917/s, 44.7917/s/gpu LR: 0.000001 Logit Scale: 74.106 Original_clip_loss: 0.16971 (0.10928) Loss: 0.16971 (0.10928) Dino_reguarizing_loss: 0.018238 (0.015146)\r\n",
      "2025-04-03,20:03:39 | INFO | Train Epoch: 7 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.095, 44.9446/s, 44.9446/s/gpu LR: 0.000001 Logit Scale: 74.109 Original_clip_loss: 0.094276 (0.10813) Loss: 0.094276 (0.10813) Dino_reguarizing_loss: 0.012516 (0.014944)\r\n",
      "2025-04-03,20:05:28 | INFO | Train Epoch: 7 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.096, 45.8016/s, 45.8016/s/gpu LR: 0.000001 Logit Scale: 74.111 Original_clip_loss: 0.18579 (0.11368) Loss: 0.18579 (0.11368) Dino_reguarizing_loss: 0.013694 (0.014854)\r\n",
      "2025-04-03,20:07:18 | INFO | Train Epoch: 7 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.096, 46.5293/s, 46.5293/s/gpu LR: 0.000001 Logit Scale: 74.113 Original_clip_loss: 0.16846 (0.11733) Loss: 0.16846 (0.11733) Dino_reguarizing_loss: 0.012173 (0.014676)\r\n",
      "2025-04-03,20:09:07 | INFO | Train Epoch: 7 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.090, 46.5843/s, 46.5843/s/gpu LR: 0.000001 Logit Scale: 74.114 Original_clip_loss: 0.096600 (0.11603) Loss: 0.096600 (0.11603) Dino_reguarizing_loss: 0.018329 (0.014904)\r\n",
      "2025-04-03,20:10:55 | INFO | Train Epoch: 7 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.078, 46.6763/s, 46.6763/s/gpu LR: 0.000001 Logit Scale: 74.114 Original_clip_loss: 0.18537 (0.12011) Loss: 0.18537 (0.12011) Dino_reguarizing_loss: 0.017288 (0.015044)\r\n",
      "2025-04-03,20:12:44 | INFO | Train Epoch: 7 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.093, 43.1474/s, 43.1474/s/gpu LR: 0.000001 Logit Scale: 74.115 Original_clip_loss: 0.099543 (0.11897) Loss: 0.099543 (0.11897) Dino_reguarizing_loss: 0.013206 (0.014942)\r\n",
      "2025-04-03,20:14:34 | INFO | Train Epoch: 7 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.096, 44.1036/s, 44.1036/s/gpu LR: 0.000001 Logit Scale: 74.116 Original_clip_loss: 0.046131 (0.11513) Loss: 0.046131 (0.11513) Dino_reguarizing_loss: 0.016491 (0.015024)\r\n",
      "2025-04-03,20:16:24 | INFO | Train Epoch: 7 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.109, 45.4872/s, 45.4872/s/gpu LR: 0.000001 Logit Scale: 74.120 Original_clip_loss: 0.27315 (0.12304) Loss: 0.27315 (0.12304) Dino_reguarizing_loss: 0.014261 (0.014985)\r\n",
      "2025-04-03,20:18:14 | INFO | Train Epoch: 7 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.094, 41.9134/s, 41.9134/s/gpu LR: 0.000001 Logit Scale: 74.120 Original_clip_loss: 0.10002 (0.12194) Loss: 0.10002 (0.12194) Dino_reguarizing_loss: 0.012911 (0.014887)\r\n",
      "2025-04-03,20:20:03 | INFO | Train Epoch: 7 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.096, 42.2889/s, 42.2889/s/gpu LR: 0.000001 Logit Scale: 74.121 Original_clip_loss: 0.11393 (0.12158) Loss: 0.11393 (0.12158) Dino_reguarizing_loss: 0.012965 (0.014799)\r\n",
      "2025-04-03,20:21:55 | INFO | Train Epoch: 7 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.116, 41.0753/s, 41.0753/s/gpu LR: 0.000001 Logit Scale: 74.121 Original_clip_loss: 0.12014 (0.12151) Loss: 0.12014 (0.12151) Dino_reguarizing_loss: 0.018257 (0.014950)\r\n",
      "2025-04-03,20:23:46 | INFO | Train Epoch: 7 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.112, 40.2686/s, 40.2686/s/gpu LR: 0.000001 Logit Scale: 74.123 Original_clip_loss: 0.090071 (0.12020) Loss: 0.090071 (0.12020) Dino_reguarizing_loss: 0.015059 (0.014954)\r\n",
      "2025-04-03,20:25:35 | INFO | Train Epoch: 7 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.084, 40.9796/s, 40.9796/s/gpu LR: 0.000001 Logit Scale: 74.123 Original_clip_loss: 0.081204 (0.11864) Loss: 0.081204 (0.11864) Dino_reguarizing_loss: 0.017363 (0.015051)\r\n",
      "2025-04-03,20:27:25 | INFO | Train Epoch: 7 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.100, 46.7525/s, 46.7525/s/gpu LR: 0.000001 Logit Scale: 74.125 Original_clip_loss: 0.10417 (0.11809) Loss: 0.10417 (0.11809) Dino_reguarizing_loss: 0.013010 (0.014972)\r\n",
      "2025-04-03,20:29:15 | INFO | Train Epoch: 7 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.105, 41.0249/s, 41.0249/s/gpu LR: 0.000001 Logit Scale: 74.126 Original_clip_loss: 0.055297 (0.11576) Loss: 0.055297 (0.11576) Dino_reguarizing_loss: 0.012085 (0.014865)\r\n",
      "2025-04-03,20:31:04 | INFO | Train Epoch: 7 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.090, 44.7098/s, 44.7098/s/gpu LR: 0.000001 Logit Scale: 74.127 Original_clip_loss: 0.11118 (0.11560) Loss: 0.11118 (0.11560) Dino_reguarizing_loss: 0.018262 (0.014987)\r\n",
      "2025-04-03,20:32:54 | INFO | Train Epoch: 7 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.095, 46.7550/s, 46.7550/s/gpu LR: 0.000001 Logit Scale: 74.128 Original_clip_loss: 0.092283 (0.11479) Loss: 0.092283 (0.11479) Dino_reguarizing_loss: 0.012765 (0.014910)\r\n",
      "2025-04-03,20:34:42 | INFO | Train Epoch: 7 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.084, 44.9744/s, 44.9744/s/gpu LR: 0.000001 Logit Scale: 74.128 Original_clip_loss: 0.12583 (0.11516) Loss: 0.12583 (0.11516) Dino_reguarizing_loss: 0.015436 (0.014927)\r\n",
      "2025-04-03,20:36:31 | INFO | Train Epoch: 7 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.087, 46.6476/s, 46.6476/s/gpu LR: 0.000001 Logit Scale: 74.130 Original_clip_loss: 0.17494 (0.11709) Loss: 0.17494 (0.11709) Dino_reguarizing_loss: 0.015694 (0.014952)\r\n",
      "2025-04-03,20:36:51 | INFO | Train Epoch: 7 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.052, 47.5695/s, 47.5695/s/gpu LR: 0.000001 Logit Scale: 74.130 Original_clip_loss: 0.22358 (0.12042) Loss: 0.22358 (0.12042) Dino_reguarizing_loss: 0.012447 (0.014874)\r\n",
      "2025-04-03,20:36:53 | INFO | Start epoch 8\r\n",
      "2025-04-03,20:36:55 | INFO | Train Epoch: 8 [    48/145000 (0%)] Data (t): 0.998 Batch (t): 2.280, 21.0501/s, 21.0501/s/gpu LR: 0.000001 Logit Scale: 74.130 Original_clip_loss: 0.077448 (0.077448) Loss: 0.077448 (0.077448) Dino_reguarizing_loss: 0.016992 (0.016992)\r\n",
      "2025-04-03,20:38:43 | INFO | Train Epoch: 8 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.085, 46.8290/s, 46.8290/s/gpu LR: 0.000000 Logit Scale: 74.130 Original_clip_loss: 0.058319 (0.067883) Loss: 0.058319 (0.067883) Dino_reguarizing_loss: 0.015353 (0.016173)\r\n",
      "2025-04-03,20:40:32 | INFO | Train Epoch: 8 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.088, 47.3904/s, 47.3904/s/gpu LR: 0.000000 Logit Scale: 74.131 Original_clip_loss: 0.060736 (0.065501) Loss: 0.060736 (0.065501) Dino_reguarizing_loss: 0.013764 (0.015370)\r\n",
      "2025-04-03,20:42:19 | INFO | Train Epoch: 8 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.070, 47.1592/s, 47.1592/s/gpu LR: 0.000000 Logit Scale: 74.131 Original_clip_loss: 0.16746 (0.090991) Loss: 0.16746 (0.090991) Dino_reguarizing_loss: 0.016547 (0.015664)\r\n",
      "2025-04-03,20:44:08 | INFO | Train Epoch: 8 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.085, 47.5315/s, 47.5315/s/gpu LR: 0.000000 Logit Scale: 74.132 Original_clip_loss: 0.067409 (0.086275) Loss: 0.067409 (0.086275) Dino_reguarizing_loss: 0.016681 (0.015867)\r\n",
      "2025-04-03,20:45:57 | INFO | Train Epoch: 8 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.094, 41.0132/s, 41.0132/s/gpu LR: 0.000000 Logit Scale: 74.132 Original_clip_loss: 0.12851 (0.093314) Loss: 0.12851 (0.093314) Dino_reguarizing_loss: 0.015873 (0.015868)\r\n",
      "2025-04-03,20:47:45 | INFO | Train Epoch: 8 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.081, 45.9230/s, 45.9230/s/gpu LR: 0.000000 Logit Scale: 74.132 Original_clip_loss: 0.14861 (0.10121) Loss: 0.14861 (0.10121) Dino_reguarizing_loss: 0.012456 (0.015381)\r\n",
      "2025-04-03,20:49:34 | INFO | Train Epoch: 8 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.088, 41.8332/s, 41.8332/s/gpu LR: 0.000000 Logit Scale: 74.133 Original_clip_loss: 0.020660 (0.091144) Loss: 0.020660 (0.091144) Dino_reguarizing_loss: 0.014085 (0.015219)\r\n",
      "2025-04-03,20:51:22 | INFO | Train Epoch: 8 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.081, 44.8295/s, 44.8295/s/gpu LR: 0.000000 Logit Scale: 74.134 Original_clip_loss: 0.18824 (0.10193) Loss: 0.18824 (0.10193) Dino_reguarizing_loss: 0.011462 (0.014801)\r\n",
      "2025-04-03,20:53:13 | INFO | Train Epoch: 8 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.106, 47.4930/s, 47.4930/s/gpu LR: 0.000000 Logit Scale: 74.134 Original_clip_loss: 0.049531 (0.096693) Loss: 0.049531 (0.096693) Dino_reguarizing_loss: 0.015881 (0.014909)\r\n",
      "2025-04-03,20:55:02 | INFO | Train Epoch: 8 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.088, 43.9872/s, 43.9872/s/gpu LR: 0.000000 Logit Scale: 74.134 Original_clip_loss: 0.18474 (0.10470) Loss: 0.18474 (0.10470) Dino_reguarizing_loss: 0.010820 (0.014538)\r\n",
      "2025-04-03,20:56:49 | INFO | Train Epoch: 8 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.077, 40.3241/s, 40.3241/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.044401 (0.099673) Loss: 0.044401 (0.099673) Dino_reguarizing_loss: 0.013716 (0.014469)\r\n",
      "2025-04-03,20:58:39 | INFO | Train Epoch: 8 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.101, 46.7319/s, 46.7319/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.074278 (0.097719) Loss: 0.074278 (0.097719) Dino_reguarizing_loss: 0.014491 (0.014471)\r\n",
      "2025-04-03,21:00:28 | INFO | Train Epoch: 8 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.088, 46.7894/s, 46.7894/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.082873 (0.096659) Loss: 0.082873 (0.096659) Dino_reguarizing_loss: 0.010668 (0.014199)\r\n",
      "2025-04-03,21:02:17 | INFO | Train Epoch: 8 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.089, 39.7559/s, 39.7559/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.078478 (0.095447) Loss: 0.078478 (0.095447) Dino_reguarizing_loss: 0.016623 (0.014361)\r\n",
      "2025-04-03,21:04:06 | INFO | Train Epoch: 8 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.094, 41.8796/s, 41.8796/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.024362 (0.091004) Loss: 0.024362 (0.091004) Dino_reguarizing_loss: 0.016738 (0.014509)\r\n",
      "2025-04-03,21:05:55 | INFO | Train Epoch: 8 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.089, 42.6112/s, 42.6112/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.12195 (0.092824) Loss: 0.12195 (0.092824) Dino_reguarizing_loss: 0.013745 (0.014464)\r\n",
      "2025-04-03,21:07:47 | INFO | Train Epoch: 8 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.114, 46.0927/s, 46.0927/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.055817 (0.090768) Loss: 0.055817 (0.090768) Dino_reguarizing_loss: 0.014636 (0.014474)\r\n",
      "2025-04-03,21:09:37 | INFO | Train Epoch: 8 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.101, 46.8725/s, 46.8725/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.17143 (0.095014) Loss: 0.17143 (0.095014) Dino_reguarizing_loss: 0.015279 (0.014516)\r\n",
      "2025-04-03,21:11:26 | INFO | Train Epoch: 8 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.094, 43.1709/s, 43.1709/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.062765 (0.093402) Loss: 0.062765 (0.093402) Dino_reguarizing_loss: 0.013952 (0.014488)\r\n",
      "2025-04-03,21:13:16 | INFO | Train Epoch: 8 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.093, 46.7652/s, 46.7652/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.14537 (0.095876) Loss: 0.14537 (0.095876) Dino_reguarizing_loss: 0.016058 (0.014563)\r\n",
      "2025-04-03,21:15:03 | INFO | Train Epoch: 8 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.077, 46.5030/s, 46.5030/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.093027 (0.095747) Loss: 0.093027 (0.095747) Dino_reguarizing_loss: 0.017674 (0.014704)\r\n",
      "2025-04-03,21:16:52 | INFO | Train Epoch: 8 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.086, 46.7526/s, 46.7526/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.18583 (0.099663) Loss: 0.18583 (0.099663) Dino_reguarizing_loss: 0.016883 (0.014799)\r\n",
      "2025-04-03,21:18:42 | INFO | Train Epoch: 8 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.096, 41.0102/s, 41.0102/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.18555 (0.10324) Loss: 0.18555 (0.10324) Dino_reguarizing_loss: 0.013664 (0.014752)\r\n",
      "2025-04-03,21:20:30 | INFO | Train Epoch: 8 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.086, 47.3899/s, 47.3899/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.066928 (0.10179) Loss: 0.066928 (0.10179) Dino_reguarizing_loss: 0.013739 (0.014711)\r\n",
      "2025-04-03,21:22:18 | INFO | Train Epoch: 8 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.080, 44.7652/s, 44.7652/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.10350 (0.10186) Loss: 0.10350 (0.10186) Dino_reguarizing_loss: 0.013629 (0.014670)\r\n",
      "2025-04-03,21:24:06 | INFO | Train Epoch: 8 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.081, 45.7915/s, 45.7915/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.076368 (0.10091) Loss: 0.076368 (0.10091) Dino_reguarizing_loss: 0.013384 (0.014622)\r\n",
      "2025-04-03,21:25:56 | INFO | Train Epoch: 8 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.098, 45.8829/s, 45.8829/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.099092 (0.10085) Loss: 0.099092 (0.10085) Dino_reguarizing_loss: 0.013605 (0.014586)\r\n",
      "2025-04-03,21:27:46 | INFO | Train Epoch: 8 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.100, 45.9514/s, 45.9514/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.062312 (0.099517) Loss: 0.062312 (0.099517) Dino_reguarizing_loss: 0.016438 (0.014649)\r\n",
      "2025-04-03,21:29:35 | INFO | Train Epoch: 8 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.087, 40.4078/s, 40.4078/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.083396 (0.098980) Loss: 0.083396 (0.098980) Dino_reguarizing_loss: 0.014607 (0.014648)\r\n",
      "2025-04-03,21:31:24 | INFO | Train Epoch: 8 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.097, 40.3973/s, 40.3973/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.055084 (0.097564) Loss: 0.055084 (0.097564) Dino_reguarizing_loss: 0.015070 (0.014662)\r\n",
      "2025-04-03,21:31:44 | INFO | Train Epoch: 8 [144960/145000 (100%)] Data (t): 0.002 Batch (t): 1.041, 48.4327/s, 48.4327/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.20964 (0.10107) Loss: 0.20964 (0.10107) Dino_reguarizing_loss: 0.014267 (0.014649)\r\n",
      "2025-04-03,21:31:46 | INFO | Start epoch 9\r\n",
      "2025-04-03,21:31:49 | INFO | Train Epoch: 9 [    48/145000 (0%)] Data (t): 1.040 Batch (t): 2.271, 21.1358/s, 21.1358/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.085608 (0.085608) Loss: 0.085608 (0.085608) Dino_reguarizing_loss: 0.015362 (0.015362)\r\n",
      "2025-04-03,21:33:38 | INFO | Train Epoch: 9 [  4848/145000 (3%)] Data (t): 0.001 Batch (t): 1.092, 43.3757/s, 43.3757/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.019788 (0.052698) Loss: 0.019788 (0.052698) Dino_reguarizing_loss: 0.017035 (0.016199)\r\n",
      "2025-04-03,21:35:27 | INFO | Train Epoch: 9 [  9648/145000 (7%)] Data (t): 0.001 Batch (t): 1.089, 44.0229/s, 44.0229/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.13694 (0.080779) Loss: 0.13694 (0.080779) Dino_reguarizing_loss: 0.017118 (0.016505)\r\n",
      "2025-04-03,21:37:16 | INFO | Train Epoch: 9 [ 14448/145000 (10%)] Data (t): 0.001 Batch (t): 1.090, 44.9429/s, 44.9429/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.014167 (0.064126) Loss: 0.014167 (0.064126) Dino_reguarizing_loss: 0.013441 (0.015739)\r\n",
      "2025-04-03,21:39:04 | INFO | Train Epoch: 9 [ 19248/145000 (13%)] Data (t): 0.001 Batch (t): 1.089, 45.8326/s, 45.8326/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.26594 (0.10449) Loss: 0.26594 (0.10449) Dino_reguarizing_loss: 0.012618 (0.015115)\r\n",
      "2025-04-03,21:40:53 | INFO | Train Epoch: 9 [ 24048/145000 (17%)] Data (t): 0.001 Batch (t): 1.086, 40.3445/s, 40.3445/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.066690 (0.098189) Loss: 0.066690 (0.098189) Dino_reguarizing_loss: 0.015076 (0.015108)\r\n",
      "2025-04-03,21:42:41 | INFO | Train Epoch: 9 [ 28848/145000 (20%)] Data (t): 0.001 Batch (t): 1.080, 40.5765/s, 40.5765/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.062789 (0.093132) Loss: 0.062789 (0.093132) Dino_reguarizing_loss: 0.014162 (0.014973)\r\n",
      "2025-04-03,21:44:30 | INFO | Train Epoch: 9 [ 33648/145000 (23%)] Data (t): 0.001 Batch (t): 1.091, 44.8844/s, 44.8844/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.10990 (0.095228) Loss: 0.10990 (0.095228) Dino_reguarizing_loss: 0.013430 (0.014780)\r\n",
      "2025-04-03,21:46:19 | INFO | Train Epoch: 9 [ 38448/145000 (27%)] Data (t): 0.001 Batch (t): 1.083, 47.5913/s, 47.5913/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.19353 (0.10615) Loss: 0.19353 (0.10615) Dino_reguarizing_loss: 0.013991 (0.014693)\r\n",
      "2025-04-03,21:48:07 | INFO | Train Epoch: 9 [ 43248/145000 (30%)] Data (t): 0.001 Batch (t): 1.086, 44.0082/s, 44.0082/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.13784 (0.10932) Loss: 0.13784 (0.10932) Dino_reguarizing_loss: 0.014442 (0.014668)\r\n",
      "2025-04-03,21:49:55 | INFO | Train Epoch: 9 [ 48048/145000 (33%)] Data (t): 0.001 Batch (t): 1.079, 46.8122/s, 46.8122/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.10373 (0.10881) Loss: 0.10373 (0.10881) Dino_reguarizing_loss: 0.016432 (0.014828)\r\n",
      "2025-04-03,21:51:44 | INFO | Train Epoch: 9 [ 52848/145000 (36%)] Data (t): 0.001 Batch (t): 1.091, 45.2873/s, 45.2873/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.045918 (0.10357) Loss: 0.045918 (0.10357) Dino_reguarizing_loss: 0.017119 (0.015019)\r\n",
      "2025-04-03,21:53:32 | INFO | Train Epoch: 9 [ 57648/145000 (40%)] Data (t): 0.001 Batch (t): 1.074, 46.5744/s, 46.5744/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.084229 (0.10208) Loss: 0.084229 (0.10208) Dino_reguarizing_loss: 0.015372 (0.015046)\r\n",
      "2025-04-03,21:55:19 | INFO | Train Epoch: 9 [ 62448/145000 (43%)] Data (t): 0.001 Batch (t): 1.076, 44.1656/s, 44.1656/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.059961 (0.099074) Loss: 0.059961 (0.099074) Dino_reguarizing_loss: 0.019613 (0.015372)\r\n",
      "2025-04-03,21:57:07 | INFO | Train Epoch: 9 [ 67248/145000 (46%)] Data (t): 0.001 Batch (t): 1.082, 43.8517/s, 43.8517/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.11818 (0.10035) Loss: 0.11818 (0.10035) Dino_reguarizing_loss: 0.014993 (0.015347)\r\n",
      "2025-04-03,21:58:55 | INFO | Train Epoch: 9 [ 72048/145000 (50%)] Data (t): 0.001 Batch (t): 1.081, 43.2910/s, 43.2910/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.11889 (0.10151) Loss: 0.11889 (0.10151) Dino_reguarizing_loss: 0.014011 (0.015263)\r\n",
      "2025-04-03,22:00:44 | INFO | Train Epoch: 9 [ 76848/145000 (53%)] Data (t): 0.001 Batch (t): 1.089, 47.4561/s, 47.4561/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.14091 (0.10382) Loss: 0.14091 (0.10382) Dino_reguarizing_loss: 0.015713 (0.015290)\r\n",
      "2025-04-03,22:02:32 | INFO | Train Epoch: 9 [ 81648/145000 (56%)] Data (t): 0.001 Batch (t): 1.080, 40.3793/s, 40.3793/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.29729 (0.11457) Loss: 0.29729 (0.11457) Dino_reguarizing_loss: 0.017310 (0.015402)\r\n",
      "2025-04-03,22:04:21 | INFO | Train Epoch: 9 [ 86448/145000 (60%)] Data (t): 0.001 Batch (t): 1.086, 40.2500/s, 40.2500/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.083036 (0.11291) Loss: 0.083036 (0.11291) Dino_reguarizing_loss: 0.013580 (0.015306)\r\n",
      "2025-04-03,22:06:07 | INFO | Train Epoch: 9 [ 91248/145000 (63%)] Data (t): 0.001 Batch (t): 1.064, 45.6756/s, 45.6756/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.042794 (0.10941) Loss: 0.042794 (0.10941) Dino_reguarizing_loss: 0.014441 (0.015263)\r\n",
      "2025-04-03,22:07:56 | INFO | Train Epoch: 9 [ 96048/145000 (66%)] Data (t): 0.001 Batch (t): 1.082, 42.3635/s, 42.3635/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.19235 (0.11336) Loss: 0.19235 (0.11336) Dino_reguarizing_loss: 0.015596 (0.015279)\r\n",
      "2025-04-03,22:09:42 | INFO | Train Epoch: 9 [100848/145000 (70%)] Data (t): 0.001 Batch (t): 1.069, 44.9788/s, 44.9788/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.11554 (0.11346) Loss: 0.11554 (0.11346) Dino_reguarizing_loss: 0.014102 (0.015225)\r\n",
      "2025-04-03,22:11:32 | INFO | Train Epoch: 9 [105648/145000 (73%)] Data (t): 0.001 Batch (t): 1.100, 39.7701/s, 39.7701/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.11521 (0.11353) Loss: 0.11521 (0.11353) Dino_reguarizing_loss: 0.011973 (0.015084)\r\n",
      "2025-04-03,22:13:22 | INFO | Train Epoch: 9 [110448/145000 (76%)] Data (t): 0.001 Batch (t): 1.099, 40.4011/s, 40.4011/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.11199 (0.11347) Loss: 0.11199 (0.11347) Dino_reguarizing_loss: 0.016278 (0.015134)\r\n",
      "2025-04-03,22:15:10 | INFO | Train Epoch: 9 [115248/145000 (80%)] Data (t): 0.001 Batch (t): 1.074, 44.7948/s, 44.7948/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.13930 (0.11450) Loss: 0.13930 (0.11450) Dino_reguarizing_loss: 0.011571 (0.014991)\r\n",
      "2025-04-03,22:16:57 | INFO | Train Epoch: 9 [120048/145000 (83%)] Data (t): 0.001 Batch (t): 1.071, 41.3338/s, 41.3338/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.013464 (0.11061) Loss: 0.013464 (0.11061) Dino_reguarizing_loss: 0.017799 (0.015099)\r\n",
      "2025-04-03,22:18:46 | INFO | Train Epoch: 9 [124848/145000 (86%)] Data (t): 0.001 Batch (t): 1.092, 44.8847/s, 44.8847/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.077104 (0.10937) Loss: 0.077104 (0.10937) Dino_reguarizing_loss: 0.0091640 (0.014879)\r\n",
      "2025-04-03,22:20:34 | INFO | Train Epoch: 9 [129648/145000 (89%)] Data (t): 0.001 Batch (t): 1.076, 47.4706/s, 47.4706/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.036249 (0.10676) Loss: 0.036249 (0.10676) Dino_reguarizing_loss: 0.016005 (0.014920)\r\n",
      "2025-04-03,22:22:22 | INFO | Train Epoch: 9 [134448/145000 (93%)] Data (t): 0.001 Batch (t): 1.079, 38.4743/s, 38.4743/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.088801 (0.10614) Loss: 0.088801 (0.10614) Dino_reguarizing_loss: 0.017986 (0.015025)\r\n",
      "2025-04-03,22:24:09 | INFO | Train Epoch: 9 [139248/145000 (96%)] Data (t): 0.001 Batch (t): 1.077, 47.5939/s, 47.5939/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.095720 (0.10580) Loss: 0.095720 (0.10580) Dino_reguarizing_loss: 0.015623 (0.015045)\r\n",
      "2025-04-03,22:25:59 | INFO | Train Epoch: 9 [144048/145000 (99%)] Data (t): 0.001 Batch (t): 1.098, 46.7741/s, 46.7741/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.081938 (0.10503) Loss: 0.081938 (0.10503) Dino_reguarizing_loss: 0.017251 (0.015116)\r\n",
      "2025-04-03,22:26:19 | INFO | Train Epoch: 9 [144960/145000 (100%)] Data (t): 0.003 Batch (t): 1.052, 48.6741/s, 48.6741/s/gpu LR: 0.000000 Logit Scale: 74.135 Original_clip_loss: 0.18950 (0.10767) Loss: 0.18950 (0.10767) Dino_reguarizing_loss: 0.015066 (0.015115)\r\n",
      "I0403 22:26:21.627000 73 torch/_dynamo/utils.py:399] TorchDynamo compilation metrics:\r\n",
      "I0403 22:26:21.627000 73 torch/_dynamo/utils.py:399] Function    Runtimes (s)\r\n",
      "I0403 22:26:21.627000 73 torch/_dynamo/utils.py:399] ----------  --------------\r\n",
      "I0403 22:26:21.634000 73 torch/_subclasses/fake_tensor.py:2423] FakeTensor cache stats:\r\n",
      "I0403 22:26:21.634000 73 torch/_subclasses/fake_tensor.py:2424]   cache_hits: 0\r\n",
      "I0403 22:26:21.635000 73 torch/_subclasses/fake_tensor.py:2425]   cache_misses: 0\r\n"
     ]
    }
   ],
   "source": [
    "# TO FINE TUNE vol1\n",
    "!python /kaggle/working/open-clip/src/open_clip_train/main.py \\\n",
    "    --train-data /kaggle/working/train_data_karpathy.csv \\\n",
    "    --name 'dino_proj_fine_tuned_Clip' \\\n",
    "    --dataset-type csv \\\n",
    "    --csv-img-key image \\\n",
    "    --csv-caption-key caption \\\n",
    "    --csv-separator \",\" \\\n",
    "    --model RN50  \\\n",
    "    --pretrained /kaggle/working/clip_final.pt \\\n",
    "    --batch-size 48 \\\n",
    "    --lr 5e-6 \\\n",
    "    --warmup 1000 \\\n",
    "    --epochs 10 \\\n",
    "    --lr-scheduler cosine \\\n",
    "    --precision amp \\\n",
    "    --workers 4 \\\n",
    "    --logs \"logs\" \\\n",
    "    --logs \"checkpoints\" \\\n",
    "    --save-frequency 1 \\\n",
    "    --seed 42 \\\n",
    "    --lambda_dino 0.25 \\\n",
    "    --alpha 0  \\\n",
    "    --use_dino_reg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e20909d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:25.966755Z",
     "iopub.status.busy": "2025-04-03T22:26:25.966396Z",
     "iopub.status.idle": "2025-04-03T22:26:25.970101Z",
     "shell.execute_reply": "2025-04-03T22:26:25.969240Z"
    },
    "papermill": {
     "duration": 0.031993,
     "end_time": "2025-04-03T22:26:25.971606",
     "exception": false,
     "start_time": "2025-04-03T22:26:25.939613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!rm -rf checkpoints/dino_proj_fine_tuned_Clip\n",
    "#!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f19850ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:26.023233Z",
     "iopub.status.busy": "2025-04-03T22:26:26.023000Z",
     "iopub.status.idle": "2025-04-03T22:26:28.821330Z",
     "shell.execute_reply": "2025-04-03T22:26:28.820243Z"
    },
    "papermill": {
     "duration": 2.825579,
     "end_time": "2025-04-03T22:26:28.822749",
     "exception": false,
     "start_time": "2025-04-03T22:26:25.997170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Path to your trained checkpoint\n",
    "checkpoint_path = \"checkpoints/dino_proj_fine_tuned_Clip/checkpoints/epoch_10.pt\"\n",
    "\n",
    "# 1. Create the model architecture from scratch (without loading pretrained weights)\n",
    "model_dino_proj_fine_tuned_Clip_1_epochs, preprocess_ft, preprocess_val = open_clip.create_model_and_transforms(\n",
    "    \"RN50\", pretrained=None\n",
    ")\n",
    "\n",
    "# 2. Load the trained checkpoint manually\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\",weights_only=True)\n",
    "\n",
    "# If the state_dict is nested under a key (like \"state_dict\"), adjust accordingly\n",
    "if \"state_dict\" in checkpoint:\n",
    "    checkpoint = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Load checkpoint weights into model\n",
    "model_dino_proj_fine_tuned_Clip_1_epochs.load_state_dict(checkpoint, strict=False)  # Use strict=False to avoid missing keys error\n",
    "\n",
    "# 3. Move model to GPU (if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_dino_proj_fine_tuned_Clip_1_epochs = model_dino_proj_fine_tuned_Clip_1_epochs.to(device)\n",
    "model_dino_proj_fine_tuned_Clip_1_epochs.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16c88e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:28.877633Z",
     "iopub.status.busy": "2025-04-03T22:26:28.877200Z",
     "iopub.status.idle": "2025-04-03T22:26:28.881127Z",
     "shell.execute_reply": "2025-04-03T22:26:28.880234Z"
    },
    "papermill": {
     "duration": 0.032212,
     "end_time": "2025-04-03T22:26:28.882516",
     "exception": false,
     "start_time": "2025-04-03T22:26:28.850304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.make_archive(\"/kaggle/working/cyclip_checkpoints\", 'zip', \"/kaggle/working/checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b19e196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:28.937843Z",
     "iopub.status.busy": "2025-04-03T22:26:28.937593Z",
     "iopub.status.idle": "2025-04-03T22:26:28.941164Z",
     "shell.execute_reply": "2025-04-03T22:26:28.940230Z"
    },
    "papermill": {
     "duration": 0.033143,
     "end_time": "2025-04-03T22:26:28.943312",
     "exception": false,
     "start_time": "2025-04-03T22:26:28.910169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#      EVALUATING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2020be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:28.994594Z",
     "iopub.status.busy": "2025-04-03T22:26:28.994366Z",
     "iopub.status.idle": "2025-04-03T22:26:29.078805Z",
     "shell.execute_reply": "2025-04-03T22:26:29.077933Z"
    },
    "papermill": {
     "duration": 0.111757,
     "end_time": "2025-04-03T22:26:29.080369",
     "exception": false,
     "start_time": "2025-04-03T22:26:28.968612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = [item for item in karpathy_data[\"images\"] if item[\"split\"] == \"test\"]\n",
    "all_captions = []\n",
    "all_images = []\n",
    "for item in test_data:\n",
    "    for sentence in item[\"sentences\"]:\n",
    "        all_captions.append(sentence[\"raw\"])\n",
    "        all_images.append(item[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c4ba345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:29.132493Z",
     "iopub.status.busy": "2025-04-03T22:26:29.132216Z",
     "iopub.status.idle": "2025-04-03T22:26:29.135669Z",
     "shell.execute_reply": "2025-04-03T22:26:29.134805Z"
    },
    "papermill": {
     "duration": 0.03078,
     "end_time": "2025-04-03T22:26:29.136904",
     "exception": false,
     "start_time": "2025-04-03T22:26:29.106124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FOR FINE TUNED CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b8eb06d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:26:29.189314Z",
     "iopub.status.busy": "2025-04-03T22:26:29.189043Z",
     "iopub.status.idle": "2025-04-03T22:27:44.351175Z",
     "shell.execute_reply": "2025-04-03T22:27:44.350262Z"
    },
    "papermill": {
     "duration": 75.189938,
     "end_time": "2025-04-03T22:27:44.352412",
     "exception": false,
     "start_time": "2025-04-03T22:26:29.162474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding batches: 100%|██████████| 105/105 [01:15<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "\n",
    "image_root = \"/kaggle/input/flickr30k/Images\"\n",
    "text_embeds, image_embeds = get_all_embeddings(\n",
    "    model_dino_proj_fine_tuned_Clip_1_epochs, all_captions, all_images, root=image_root,\n",
    "    preprocess=preprocess, tokenizer=tokenizer, device=device , batch_size=48\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1b3c18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:44.414633Z",
     "iopub.status.busy": "2025-04-03T22:27:44.414364Z",
     "iopub.status.idle": "2025-04-03T22:27:45.674396Z",
     "shell.execute_reply": "2025-04-03T22:27:45.673332Z"
    },
    "papermill": {
     "duration": 1.292098,
     "end_time": "2025-04-03T22:27:45.675736",
     "exception": false,
     "start_time": "2025-04-03T22:27:44.383638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation Results:\n",
      "txt_r1: 91.56\n",
      "txt_r5: 96.76\n",
      "txt_r10: 98.14\n",
      "txt_r_mean: 95.49\n",
      "img_r1: 42.78\n",
      "img_r5: 73.66\n",
      "img_r10: 82.54\n",
      "img_r_mean: 66.33\n",
      "r_mean: 80.91\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "results = itm_eval(text_embeds, image_embeds)\n",
    "print(\"📊 Evaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "638807c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:45.739172Z",
     "iopub.status.busy": "2025-04-03T22:27:45.738877Z",
     "iopub.status.idle": "2025-04-03T22:27:45.741873Z",
     "shell.execute_reply": "2025-04-03T22:27:45.741175Z"
    },
    "papermill": {
     "duration": 0.035759,
     "end_time": "2025-04-03T22:27:45.743261",
     "exception": false,
     "start_time": "2025-04-03T22:27:45.707502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONSISTENCY METRICS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83ca0def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:45.803333Z",
     "iopub.status.busy": "2025-04-03T22:27:45.803065Z",
     "iopub.status.idle": "2025-04-03T22:27:45.814564Z",
     "shell.execute_reply": "2025-04-03T22:27:45.813726Z"
    },
    "papermill": {
     "duration": 0.042761,
     "end_time": "2025-04-03T22:27:45.815854",
     "exception": false,
     "start_time": "2025-04-03T22:27:45.773093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_records = []\n",
    "for item in test_data:\n",
    "    img_filename = f\"/kaggle/input/flickr30k/Images/{item['filename']}\"\n",
    "    for sentence in item[\"sentences\"]:\n",
    "        caption = sentence[\"raw\"]\n",
    "        test_records.append({\"image\": img_filename, \"caption\": caption})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3fab7fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:45.879188Z",
     "iopub.status.busy": "2025-04-03T22:27:45.878855Z",
     "iopub.status.idle": "2025-04-03T22:27:45.883784Z",
     "shell.execute_reply": "2025-04-03T22:27:45.883064Z"
    },
    "papermill": {
     "duration": 0.038088,
     "end_time": "2025-04-03T22:27:45.885242",
     "exception": false,
     "start_time": "2025-04-03T22:27:45.847154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CyCLIPDataset(Dataset):\n",
    "    def __init__(self, records, image_transform):\n",
    "        self.records = records\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self.records[idx]\n",
    "        image = self.image_transform(Image.open(record[\"image\"]).convert(\"RGB\"))\n",
    "        text = tokenizer(record[\"caption\"]).squeeze(0)  # remove batch dim\n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39311608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:45.947707Z",
     "iopub.status.busy": "2025-04-03T22:27:45.947423Z",
     "iopub.status.idle": "2025-04-03T22:27:45.951877Z",
     "shell.execute_reply": "2025-04-03T22:27:45.950978Z"
    },
    "papermill": {
     "duration": 0.037091,
     "end_time": "2025-04-03T22:27:45.953273",
     "exception": false,
     "start_time": "2025-04-03T22:27:45.916182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = CyCLIPDataset(test_records, preprocess)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02b854a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:27:46.013290Z",
     "iopub.status.busy": "2025-04-03T22:27:46.013058Z",
     "iopub.status.idle": "2025-04-03T22:28:53.053371Z",
     "shell.execute_reply": "2025-04-03T22:28:53.052540Z"
    },
    "papermill": {
     "duration": 67.103945,
     "end_time": "2025-04-03T22:28:53.087167",
     "exception": false,
     "start_time": "2025-04-03T22:27:45.983222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency Score: 0.4791\n"
     ]
    }
   ],
   "source": [
    "# compute_consistency_score calculate mean of similarities\n",
    "\n",
    "# CLIP final with og Fine tuning\n",
    "\n",
    "score = compute_consistency_score(model_dino_proj_fine_tuned_Clip_1_epochs, test_loader, device)\n",
    "\n",
    "print(f\"Consistency Score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623329,
     "sourceId": 1111749,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1011404,
     "sourceId": 1706129,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7006240,
     "sourceId": 11219022,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7006413,
     "sourceId": 11219272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7029845,
     "sourceId": 11250022,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33152.559797,
   "end_time": "2025-04-03T22:28:56.598084",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-03T13:16:24.038287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
